# 模型功能

## 嵌入模型
- 使用 `ChatLaw-Text2Vec` 构建法律知识库。
- 支持本地离线运行，保障隐私与稳定性。

## Rank 重排序模型
- 默认可选 `Qwen3-Reranker-0.6B` 本地 CrossEncoder。
- 通过 `SimpleQwenReranker` 逐条评分，筛选最相关条文。
- 可在侧边栏启用/关闭，并显示当前状态与可用内存。

## 对话大模型（LLM）
- 支持 **DeepSeek**、**智谱 GLM**、**本地 OpenAI 接口兼容模型**。
- DeepSeek、GLM 均内置多个子模型，可针对推理/生成需求选择。
- 自动检测 API Key、回退到备用模型或本地模型，提升鲁棒性。


## DeepSeek 子模型功能介绍

| 配置键名           | 引用的 `model` 字段值 | 官方支持 | 主要特点与说明                                                                 |
|--------------------|-----------------------|----------|-------------------------------------------------------------------------------|
| 基础对话模型       | `deepseek-chat`       | ✅ 是     | 通用的对话模型，适合大多数文本生成和对话任务。                               |
| 推理模型           | `deepseek-reasoner`   | ✅ 是     | 具备深度思考（Chain-of-Thought）能力的推理模型，擅长复杂逻辑与数学问题。     |
| 最新 V3 系列       | `deepseek-v3`         | ✅ 是     | 最新的高性能基础模型，支持思考与非思考两种模式，可通过 API 参数 `thinking` 控制。 |
| 强化推理模型       | `deepseek-r1`         | ✅ 是     | 基于 V3 架构的强化推理模型，专为深度推理和复杂任务优化。                     |


## GLM 子模型功能介绍

| 模型系列          | 可直接引用的 `model` 字段值 | 官方支持 | 主要特点或备注                                   |
|-------------------|-----------------------------|----------|--------------------------------------------------|
| 基础与综合        | `glm-4`                     | ✅ 是     | 均衡的综合能力模型。                             |
|                   | `glm-4-plus`               | ✅ 是     | 高性能旗舰模型。                                 |
|                   | `glm-4-long`               | ✅ 是     | 专注于 128K 超长上下文处理。                     |
|                   | `glm-4-flash`              | ✅ 是     | 追求极速与性价比的轻量版本。                     |
| GLM-4.5 系列      | `glm-4-5`                   | ✅ 是     | 全能智能体模型，支持思考 / 非思考模式。          |
|                   | `glm-4-5-air`              | ✅ 是     | GLM-4.5 的轻量高效版本。                         |
| GLM-4.6 系列      | `glm-4-6`                   | ✅ 是     | 当前最强代码模型，支持 200K 上下文。             |
|                   | `glm-4-6-9b`               | ✅ 是     | 专为边缘设备优化的 90 亿参数轻量版。             |
| Z1 推理系列       | `glm-z1-air`               | ✅ 是     | 具备深度思考能力的推理模型。                     |
|                   | `glm-z1-flash`             | ✅ 是     | 免费使用的推理模型。                             |
| 多模态            | `glm-4v`                    | ✅ 是     | 视觉语言模型，支持图像理解。                     |


## Qwen 子模型功能介绍

| 系列               | 描述                                           | 包含的模型配置数量 |
|--------------------|-----------------------------------------------|--------------------|
| Max 系列           | 性能最强的旗舰模型，适合高复杂度任务。         | 4                  |
| Plus 系列          | 均衡的高性能模型，兼顾能力与成本。             | 3                  |
| Flash/Turbo 系列   | 极速响应、低成本的模型，适合高频或实时交互。   | 4                  |
| 长上下文系列       | 支持超长文本输入，专为长文档处理优化。         | 2                  |
| Coder/数学等专用模型 | 为代码生成、数学推理等专项任务优化的模型。     | 5                  |
| 开源指令模型       | 开源的指令调优模型，提供高自由度。             | 2                  |

### Qwen 模型说明表

| 系列               | 模型名称 (`model` 字段值)                                                                 | 核心定位与特点                                                                                         | 计费说明（一般规律）                                                   |
|--------------------|--------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------|
| Max 系列           | `qwen3-max`、`qwen3-max-2025-09-23`、`qwen3-max-preview`、`qwen-max`                       | 旗舰性能，参数规模最大，在复杂推理、指令遵循、创意生成等任务上表现最强。                               | 价格最高，适合对输出质量要求极致的场景。                               |
| Plus 系列          | `qwen-plus`、`qwen-plus-latest`、`qwen-plus-2025-09-11`                                    | 均衡高性能，在能力、速度和成本间取得最佳平衡，是大多数复杂任务的性价比之选。                         | 价格中等偏高，低于 Max 系列，高于轻量系列。                            |
| Flash/Turbo 系列   | `qwen-flash`、`qwen-flash-2025-07-28`、`qwen-turbo`、`qwen-turbo-latest`                   | 极速低成本，响应速度最快、单位 Token 成本最低，适合高并发、实时交互或简单任务。                       | 价格最低，主打性价比和高频调用场景。                                   |
| 长上下文系列       | `qwen-long`、`qwen-plus-long`                                                              | 支持超长文本输入，`qwen-long` 可到百万上下文，适合长文档总结、知识库问答等场景。                       | 通常按有效上下文长度阶梯计费，处理超长文本时需关注具体定价。           |
| Coder/数学专用模型 | `qwen3-coder-plus`、`qwen3-coder-plus-2025-09-23`、`qwen3-coder-flash`、`qwen-coder-plus`、`qwen-math-plus` | 垂直领域优化：Coder 系列专注代码生成与补全；Math 系列强化数学推理能力。                               | 通常略高于同等级通用模型，为专项能力付费。                             |
| 开源指令模型       | `qwen2-72b-instruct`、`qwen2.5-32b-instruct`                                               | 开源可定制模型，适合需要深度定制、微调或对成本极其敏感的场景。                                         | 成本通常低于闭源商业模型，取决于部署平台和算力资源。                   |

💡 重要说明与建议
计费为通用规律，具体需查证：上表中的“计费说明”是基于模型定位的一般规律描述。实际价格会变动，务必在使用前查阅 阿里云百炼官方定价页 获取准确信息。

- 模型版本差异：带日期后缀（如 -2025-09-23）的是特定版本快照，行为一致且稳定；-latest 或无后缀指向最新迭代版本，可能持续优化但偶有变动。
- 上下文长度核实：配置中 qwen-long 的 1,000,000 tokens 是极高规格，请在官方文档中确认其完全可用性、调用方式及是否产生额外费用。
- 如何选择模型：

1.追求顶级效果 -> Max系列;

2.兼顾质量与成本 -> Plus系列;

3.需要高速响应/处理简单任务 -> Flash/Turbo系列。

4.总结长文档、书籍 -> 长上下文系列

5.开发编程助手、解数学题 -> Coder/数学专用系列

6.需要模型微调或控制成本 -> 开源指令模型