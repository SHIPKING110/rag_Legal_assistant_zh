# -*- coding: utf-8 -*-
"""
èŠå¤©æ¨¡å‹æ¨¡å—
å¤„ç†ç”¨æˆ·è¾“å…¥ã€æ³•å¾‹åˆ¤æ–­ã€æ£€ç´¢ã€ç”Ÿæˆå›ç­”ç­‰æ ¸å¿ƒèŠå¤©åŠŸèƒ½
"""
import re
import time
import traceback
from pathlib import Path
from typing import List

import streamlit as st
from llama_index.core import get_response_synthesizer
from llama_index.llms.huggingface import HuggingFaceLLM

from utils import Config, LLM_CONFIGS


def init_chat_interface():
    """åˆå§‹åŒ–èŠå¤©ç•Œé¢ï¼Œæ˜¾ç¤ºå†å²æ¶ˆæ¯"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    for msg in st.session_state.messages:
        role = msg["role"]
        content = msg.get("cleaned", msg["content"])  # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹
        
        with st.chat_message(role):
            st.markdown(content)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å«æ€ç»´é“¾
            if role == "assistant" and msg.get("think"):
                with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆå†å²å¯¹è¯ï¼‰"):
                    for think_content in msg["think"]:
                        st.markdown(f'<span style="color: #808080">{think_content.strip()}</span>',
                                  unsafe_allow_html=True)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”æœ‰å‚è€ƒä¾æ®ï¼ˆéœ€è¦ä¿æŒåŸæœ‰å‚è€ƒä¾æ®é€»è¾‘ï¼‰
            if role == "assistant" and "reference_nodes" in msg:
                show_reference_details(msg["reference_nodes"])


def show_reference_details(nodes):
    """æ˜¾ç¤ºå‚è€ƒä¾æ®è¯¦æƒ…"""
    with st.expander("æŸ¥çœ‹æ”¯æŒä¾æ®"):
        for idx, node in enumerate(nodes, 1):
            meta = node.node.metadata
            st.markdown(f"**[{idx}] {meta['full_title']}**")
            st.caption(f"æ¥æºæ–‡ä»¶ï¼š{meta['source_file']} | æ³•å¾‹åç§°ï¼š{meta['law_name']}")
            st.markdown(f"ç›¸å…³åº¦ï¼š`{node.score:.4f}`")
            st.info(f"{node.node.text}")


def synthesize_with_retries(synthesizer, prompt: str, nodes: List, retries: int = 3, initial_delay: float = 2.0):
    """å¯¹ response_synthesizer.synthesize æ·»åŠ æœ‰é™é‡è¯•å’ŒæŒ‡æ•°é€€é¿ã€‚

    å‚æ•°:
        synthesizer: response_synthesizer å®ä¾‹
        prompt: ç”¨æˆ·è¾“å…¥
        nodes: ç”¨äºåˆæˆçš„èŠ‚ç‚¹åˆ—è¡¨
        retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        initial_delay: åˆå§‹ç­‰å¾…ç§’æ•°ï¼Œåç»­æŒ‰ 2^n æŒ‡æ•°å¢é•¿
    è¿”å›:
        åˆæˆå™¨è¿”å›çš„å¯¹è±¡ï¼ˆä¸åŸ synthesize è¿”å›ç›¸åŒï¼‰
    æŠ›å‡º:
        æœ€åä¸€æ¬¡å¼‚å¸¸ï¼ˆè‹¥å…¨éƒ¨é‡è¯•å¤±è´¥ï¼‰
    """
    last_exc = None
    for attempt in range(1, retries + 1):
        try:
            return synthesizer.synthesize(prompt, nodes=nodes)
        except Exception as e:
            last_exc = e
            print(f"[synthesize_with_retries] å°è¯• {attempt}/{retries} å¤±è´¥: {e}")
            traceback.print_exc()
            if attempt == retries:
                # é‡è¯•å®Œæ¯•ï¼Œé‡å¤æŠ›å‡ºæœ€åçš„å¼‚å¸¸
                raise
            # æŒ‡æ•°é€€é¿
            wait = initial_delay * (2 ** (attempt - 1))
            print(f"[synthesize_with_retries] ç­‰å¾… {wait}s åé‡è¯•...")
            time.sleep(wait)


def is_legal_related(question: str, llm) -> bool:
    """åˆ¤æ–­ç”¨æˆ·é—®é¢˜æ˜¯å¦ä¸æ³•å¾‹ç›¸å…³
    
    å‚æ•°:
        question: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        llm: LLMå®ä¾‹ï¼Œç”¨äºåˆ¤æ–­
    è¿”å›:
        True: ä¸æ³•å¾‹ç›¸å…³ï¼Œéœ€è¦å¯ç”¨æ£€ç´¢
        False: ä¸æ³•å¾‹æ— å…³ï¼Œç›´æ¥ä½¿ç”¨å¯¹è¯æ¨¡å‹å›ç­”
    """
    try:
        # æ„å»ºåˆ¤æ–­æç¤º
        judgment_prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹é—®é¢˜æ˜¯å¦ä¸æ³•å¾‹ã€æ³•è§„ã€æ³•å¾‹å’¨è¯¢ã€æ³•å¾‹é—®é¢˜ç›¸å…³ã€‚

é—®é¢˜ï¼š{question}

è¯·åªå›ç­”"æ˜¯"æˆ–"å¦"ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

å¦‚æœé—®é¢˜æ¶‰åŠï¼š
- æ³•å¾‹æ³•è§„ã€æ³•å¾‹æ¡æ–‡ã€æ³•å¾‹æ¡æ¬¾
- æ³•å¾‹å’¨è¯¢ã€æ³•å¾‹é—®é¢˜ã€æ³•å¾‹çº çº·
- åˆåŒã€åè®®ã€æ³•å¾‹æ–‡ä»¶
- è¯‰è®¼ã€ä»²è£ã€æ³•å¾‹ç¨‹åº
- æ³•å¾‹æƒåˆ©ã€æ³•å¾‹ä¹‰åŠ¡ã€æ³•å¾‹è´£ä»»
- ä»»ä½•éœ€è¦å‚è€ƒæ³•å¾‹æ¡æ–‡æ¥å›ç­”çš„é—®é¢˜

è¯·å›ç­”"æ˜¯"ã€‚

å¦‚æœé—®é¢˜æ˜¯ä¸€èˆ¬æ€§å¯¹è¯ã€é—²èŠã€éæ³•å¾‹ç›¸å…³çš„æŠ€æœ¯é—®é¢˜ã€ç”Ÿæ´»å¸¸è¯†ç­‰ï¼Œè¯·å›ç­”"å¦"ã€‚

å›ç­”ï¼š"""
        
        # è°ƒç”¨LLMè¿›è¡Œåˆ¤æ–­
        response = llm.complete(judgment_prompt)
        result = response.text.strip().lower()
        
        # è§£æç»“æœ
        if "æ˜¯" in result or "yes" in result or "true" in result or "1" in result:
            print(f"[is_legal_related] åˆ¤æ–­ç»“æœï¼šä¸æ³•å¾‹ç›¸å…³")
            return True
        else:
            print(f"[is_legal_related] åˆ¤æ–­ç»“æœï¼šä¸æ³•å¾‹æ— å…³")
            return False
            
    except Exception as e:
        # å¦‚æœåˆ¤æ–­å¤±è´¥ï¼Œé»˜è®¤å¯ç”¨æ£€ç´¢ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        print(f"[is_legal_related] åˆ¤æ–­å¤±è´¥: {e}ï¼Œé»˜è®¤å¯ç”¨æ£€ç´¢")
        traceback.print_exc()
        return True


def handle_chat_message(
    prompt: str,
    retriever,
    response_synthesizer,
    llm_choice: str,
    min_rerank_score: float,
    try_auto_switch_llm_func
):
    """å¤„ç†ç”¨æˆ·èŠå¤©æ¶ˆæ¯
    
    å‚æ•°:
        prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        retriever: æ£€ç´¢å™¨å®ä¾‹
        response_synthesizer: å“åº”åˆæˆå™¨å®ä¾‹
        llm_choice: å½“å‰é€‰æ‹©çš„LLMæ¨¡å‹
        min_rerank_score: æœ€å°é‡æ’åºåˆ†æ•°é˜ˆå€¼
        try_auto_switch_llm_func: è‡ªåŠ¨åˆ‡æ¢LLMçš„å‡½æ•°
    è¿”å›:
        tuple: (response_text, filtered_nodes, used_rank)
    """
    start_time = time.time()
    used_rank = False
    
    # é¦–å…ˆåˆ¤æ–­é—®é¢˜æ˜¯å¦ä¸æ³•å¾‹ç›¸å…³
    llm = st.session_state.get("llm")
    if llm is None:
        st.error("âŒ LLMæœªåˆå§‹åŒ–")
        st.stop()
    
    is_legal = is_legal_related(prompt, llm)
    
    if not is_legal:
        # ä¸æ³•å¾‹æ— å…³ï¼Œç›´æ¥ä½¿ç”¨å¯¹è¯æ¨¡å‹å›ç­”
        st.info("ğŸ’¬ æ£€æµ‹åˆ°é—®é¢˜ä¸æ³•å¾‹æ— å…³ï¼Œä½¿ç”¨å¯¹è¯æ¨¡å¼å›ç­”")
        try:
            response = llm.complete(prompt)
            response_text = response.text
            filtered_nodes = []  # éæ³•å¾‹é—®é¢˜æ²¡æœ‰å‚è€ƒä¾æ®
        except Exception as e:
            print(f"[handle_chat_message] ç›´æ¥å¯¹è¯æ¨¡å¼å¤±è´¥: {e}")
            traceback.print_exc()
            response_text = f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}"
            filtered_nodes = []
    else:
        # ä¸æ³•å¾‹ç›¸å…³ï¼Œå¯ç”¨æ£€ç´¢æµç¨‹
        st.info("âš–ï¸ æ£€æµ‹åˆ°é—®é¢˜ä¸æ³•å¾‹ç›¸å…³ï¼Œå¯ç”¨æ³•å¾‹æ£€ç´¢æ¨¡å¼")
        
        # æ£€ç´¢æµç¨‹
        initial_nodes = retriever.retrieve(prompt)
        
        # ä½¿ç”¨ä¼šè¯çŠ¶æ€ä¸­çš„ rerankerï¼ˆä»…åœ¨å¯ç”¨ä¸”å·²åŠ è½½æ—¶ä½¿ç”¨ï¼‰
        reranker = st.session_state.reranker
        enable_rank = st.session_state.get("enable_rank_model", False)
        
        if enable_rank and reranker is not None and hasattr(reranker, 'is_loaded') and reranker.is_loaded():
            try:
                reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=prompt)
                # è¿‡æ»¤èŠ‚ç‚¹
                filtered_nodes = [node for node in reranked_nodes if node.score > min_rerank_score]
                st.success("âœ… å·²ä½¿ç”¨é‡æ’åºåŠŸèƒ½")
                used_rank = True
            except Exception as e:
                st.warning(f"âš ï¸ é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€æ£€ç´¢ç»“æœ")
                # å›é€€åˆ°æŒ‰æ£€ç´¢ç›¸ä¼¼åº¦æ’åºçš„å‰ TOP_K æ¡
                filtered_nodes = initial_nodes[:Config.TOP_K]
        else:
            # å¦‚æœæ²¡æœ‰å¯ç”¨é‡æ’åºæ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨åˆå§‹èŠ‚ç‚¹
            st.info("âš ï¸ Rankæ¨¡å‹æœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€æ£€ç´¢ç»“æœ")
            filtered_nodes = initial_nodes[:Config.TOP_K]  # ä½¿ç”¨æ£€ç´¢å¾—åˆ°çš„å‰ TOP_K æ¡
        
        if not filtered_nodes:
            response_text = "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼Œè¯·å°è¯•è°ƒæ•´é—®é¢˜æè¿°æˆ–å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆã€‚"
        else:
            # æ„é€ å¸¦æœ‰æ³•å¾‹RAGæç¤ºè¯çš„ç³»ç»Ÿæç¤º
            legal_prompt_text = ""
            try:
                legal_prompt_path = Path(Config.LEGAL_CHAT_PROMPT_PATH)
                if legal_prompt_path.exists():
                    legal_prompt_text = legal_prompt_path.read_text(encoding="utf-8")
            except Exception as e:
                print(f"[handle_chat_message] è¯»å–æ³•å¾‹æç¤ºè¯æ¨¡ç‰ˆå¤±è´¥: {e}")

            if legal_prompt_text:
                full_prompt = f"{legal_prompt_text}\n\nç”¨æˆ·é—®é¢˜ï¼š{prompt}"
            else:
                full_prompt = prompt

            # ç”Ÿæˆå›ç­”ï¼ˆå®‰å…¨è°ƒç”¨ï¼šå¸¦é‡è¯•ä¸å›é€€ï¼‰
            try:
                response = synthesize_with_retries(response_synthesizer, full_prompt, filtered_nodes, retries=3)
                response_text = response.response
            except Exception as e:
                # æ‰“å°è¯¦ç»†è·Ÿè¸ªä»¥ä¾¿è°ƒè¯•
                print("[handle_chat_message] response_synthesizer ç”Ÿæˆå¤±è´¥ï¼Œè¿›å…¥å›é€€é€»è¾‘:")
                traceback.print_exc()
                # å‘ç”¨æˆ·æ˜¾ç¤ºå‹å¥½æç¤º
                st.error("âš ï¸ åç«¯æ¨¡å‹æœåŠ¡å¼‚å¸¸ï¼Œæ­£åœ¨å°è¯•åˆ‡æ¢å¤‡ç”¨æ¨¡å‹æˆ–å›é€€ä¸ºä¸´æ—¶ç»“æœã€‚")

                # ä¼˜å…ˆå°è¯•è‡ªåŠ¨åˆ‡æ¢åˆ°å…¶å®ƒå¯ç”¨æ¨¡å‹å¹¶é‡è¯•ä¸€æ¬¡
                switched = False
                try:
                    switched = try_auto_switch_llm_func(st.session_state.get('current_llm_choice', llm_choice))
                except Exception as e_switch:
                    print(f"[handle_chat_message] è‡ªåŠ¨åˆ‡æ¢æ¨¡å‹è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e_switch}")

                if switched:
                    try:
                        # ä½¿ç”¨æ–°çš„ LLM é‡æ–°åˆ›å»ºåˆæˆå™¨å¹¶é‡è¯•
                        response_synthesizer = get_response_synthesizer(verbose=True)
                        response = synthesize_with_retries(response_synthesizer, prompt, filtered_nodes, retries=2)
                        response_text = response.response
                        # å¦‚æœæˆåŠŸåˆ™è·³è¿‡åç»­å›é€€é€»è¾‘
                    except Exception as e2:
                        print("[handle_chat_message] åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹åé‡è¯•ä»å¤±è´¥:", e2)
                        traceback.print_exc()
                        switched = False

                if not switched:
                    # å°†å‰3æ¡æ£€ç´¢åˆ°çš„æ–‡æ¡£æ‹¼æ¥ä¸ºä¸´æ—¶å†…å®¹
                    concatenated = "\n\n".join([n.node.text for n in filtered_nodes[:3]])

                    # å°è¯•ä½¿ç”¨æœ¬åœ°å°æ¨¡å‹åšå¿«é€Ÿæ‘˜è¦ï¼ˆå¦‚æœé…ç½®å¹¶å­˜åœ¨æœ¬åœ°æ¨¡å‹ï¼‰
                    summary_text = None
                    try:
                        local_cfg = LLM_CONFIGS.get("local")
                        if local_cfg:
                            local_model_path = local_cfg.get("model")
                            if local_model_path and Path(local_model_path).exists():
                                try:
                                    hf_llm = HuggingFaceLLM(model_name=str(local_model_path), temperature=0.2, max_length=256)
                                    # ä½¿ç”¨hf_llmè¿›è¡Œå¿«é€Ÿæ‘˜è¦
                                    summary_text = hf_llm.predict(f"è¯·ç®€è¦æ€»ç»“ä»¥ä¸‹æ³•å¾‹æ¡æ–‡è¦ç‚¹ï¼š\n\n{concatenated}\n\næ€»ç»“ï¼š")
                                except Exception as e_local:
                                    print(f"[fallback] æœ¬åœ°æ¨¡å‹æ‘˜è¦å¤±è´¥: {e_local}")
                    except Exception as e_cfg:
                        print(f"[fallback] æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e_cfg}")

                    if summary_text:
                        cleaned_response = summary_text
                        response_text = f"âš ï¸ åç«¯æœåŠ¡å¼‚å¸¸ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆçš„ä¸´æ—¶æ‘˜è¦ï¼š\n\n{summary_text}"
                    else:
                        # å›é€€åˆ°æ‹¼æ¥çš„åŸæ–‡
                        cleaned_response = concatenated
                        response_text = f"âš ï¸ åç«¯æ¨¡å‹æœåŠ¡å¼‚å¸¸ï¼š{e}\n\nç›¸å…³æ¡æ–‡ï¼ˆä¸´æ—¶ç»“æœï¼‰ï¼š\n{concatenated}"
    
    return response_text, filtered_nodes, used_rank


def display_chat_response(response_text: str, filtered_nodes: List, used_rank: bool):
    """æ˜¾ç¤ºèŠå¤©å“åº”
    
    å‚æ•°:
        response_text: å“åº”æ–‡æœ¬
        filtered_nodes: è¿‡æ»¤åçš„èŠ‚ç‚¹åˆ—è¡¨
        used_rank: æ˜¯å¦ä½¿ç”¨äº†Rankæ¨¡å‹
    """
    # æå–æ€ç»´é“¾å†…å®¹å¹¶æ¸…ç†å“åº”æ–‡æœ¬
    think_contents = re.findall(r'<think>(.*?)</think>', response_text, re.DOTALL)
    cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()
    
    # æ˜¾ç¤ºå›ç­”
    with st.chat_message("assistant"):
        # æ˜¾ç¤ºæ¸…ç†åçš„å›ç­”
        st.markdown(cleaned_response)
        
        # å¦‚æœæœ‰æ€ç»´é“¾å†…å®¹åˆ™æ˜¾ç¤º
        if think_contents:
            with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                for content in think_contents:
                    st.markdown(f'<span style="color: #808080">{content.strip()}</span>', 
                              unsafe_allow_html=True)
        
        # ä»…åœ¨æœ‰å‚è€ƒä¾æ®æ—¶æ˜¾ç¤ºï¼ˆæ³•å¾‹ç›¸å…³é—®é¢˜æ‰æœ‰å‚è€ƒä¾æ®ï¼‰
        if filtered_nodes:
            # å±•ç¤ºæ•°é‡ä¸æ£€ç´¢/é‡æ’åºè®¾ç½®è”åŠ¨ï¼š
            # - å¯ç”¨å¹¶æˆåŠŸä½¿ç”¨ Rank æ—¶ï¼šæœ€å¤šå±•ç¤º RERANK_TOP_K æ¡
            # - æœªå¯ç”¨ Rank æ—¶ï¼šå±•ç¤ºæ‰€æœ‰æ£€ç´¢å¾—åˆ°çš„æ¡æ–‡ï¼ˆå·²æŒ‰ TOP_K æˆªæ–­ï¼‰
            if used_rank:
                ref_k = min(Config.RERANK_TOP_K, len(filtered_nodes))
            else:
                ref_k = len(filtered_nodes)
            show_reference_details(filtered_nodes[:ref_k])
    
    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²ï¼ˆéœ€è¦å­˜å‚¨åŸå§‹å“åº”ï¼‰
    if filtered_nodes:
        if used_rank:
            ref_k = min(Config.RERANK_TOP_K, len(filtered_nodes))
        else:
            ref_k = len(filtered_nodes)
    else:
        ref_k = 0
    
    st.session_state.messages.append({
        "role": "assistant",
        "content": response_text,  # ä¿ç•™åŸå§‹å“åº”
        "cleaned": cleaned_response,  # å­˜å‚¨æ¸…ç†åçš„æ–‡æœ¬
        "think": think_contents,  # å­˜å‚¨æ€ç»´é“¾å†…å®¹
        "reference_nodes": filtered_nodes[:ref_k] if filtered_nodes else []  # å­˜å‚¨å‚è€ƒèŠ‚ç‚¹
    })

