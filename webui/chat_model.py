# -*- coding: utf-8 -*-
"""
èŠå¤©æ¨¡å‹æ¨¡å—
å¤„ç†ç”¨æˆ·è¾“å…¥ã€æ³•å¾‹åˆ¤æ–­ã€æ£€ç´¢ã€ç”Ÿæˆå›ç­”ç­‰æ ¸å¿ƒèŠå¤©åŠŸèƒ½
"""
import re
import time
import traceback
from pathlib import Path
from typing import List, Optional

import streamlit as st
from llama_index.core import get_response_synthesizer
from llama_index.llms.huggingface import HuggingFaceLLM

from utils import Config, LLM_CONFIGS

# çŸ­æœŸè®°å¿†é…ç½®
MAX_HISTORY_TURNS = 5  # æœ€å¤šä¿ç•™æœ€è¿‘5è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡


def build_conversation_context(messages: List[dict], max_turns: int = MAX_HISTORY_TURNS) -> str:
    """æ„å»ºå¯¹è¯å†å²ä¸Šä¸‹æ–‡
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        max_turns: æœ€å¤§ä¿ç•™è½®æ•°
    
    Returns:
        æ ¼å¼åŒ–çš„å¯¹è¯å†å²å­—ç¬¦ä¸²
    """
    if not messages:
        return ""
    
    # è·å–æœ€è¿‘çš„å¯¹è¯ï¼ˆæ¯è½®åŒ…å«ç”¨æˆ·å’ŒåŠ©æ‰‹å„ä¸€æ¡æ¶ˆæ¯ï¼‰
    recent_messages = messages[-(max_turns * 2):]
    
    if not recent_messages:
        return ""
    
    context_parts = []
    for msg in recent_messages:
        role = msg.get("role", "")
        # ä½¿ç”¨æ¸…ç†åçš„å†…å®¹ï¼Œé¿å…åŒ…å«æ€ç»´é“¾ç­‰
        content = msg.get("cleaned", msg.get("content", ""))
        
        if role == "user":
            context_parts.append(f"ç”¨æˆ·: {content}")
        elif role == "assistant":
            # æˆªæ–­è¿‡é•¿çš„å›å¤
            if len(content) > 500:
                content = content[:500] + "..."
            context_parts.append(f"åŠ©æ‰‹: {content}")
    
    return "\n".join(context_parts)


def build_prompt_with_history(current_prompt: str, messages: List[dict]) -> str:
    """æ„å»ºåŒ…å«å†å²å¯¹è¯çš„å®Œæ•´æç¤º
    
    Args:
        current_prompt: å½“å‰ç”¨æˆ·è¾“å…¥
        messages: å†å²æ¶ˆæ¯åˆ—è¡¨
    
    Returns:
        åŒ…å«å†å²ä¸Šä¸‹æ–‡çš„å®Œæ•´æç¤º
    """
    # æ’é™¤å½“å‰æ¶ˆæ¯ï¼ˆå› ä¸ºå½“å‰æ¶ˆæ¯è¿˜æ²¡æœ‰æ·»åŠ åˆ°å†å²ä¸­ï¼‰
    history_context = build_conversation_context(messages)
    
    if history_context:
        return f"""ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯å†å²ï¼Œè¯·å‚è€ƒè¿™äº›ä¸Šä¸‹æ–‡æ¥å›ç­”ç”¨æˆ·çš„æ–°é—®é¢˜ï¼š

ã€å¯¹è¯å†å²ã€‘
{history_context}

ã€å½“å‰é—®é¢˜ã€‘
{current_prompt}

è¯·æ ¹æ®å¯¹è¯å†å²çš„ä¸Šä¸‹æ–‡ï¼Œå›ç­”ç”¨æˆ·çš„å½“å‰é—®é¢˜ã€‚å¦‚æœå½“å‰é—®é¢˜ä¸ä¹‹å‰çš„å¯¹è¯ç›¸å…³ï¼Œè¯·ä¿æŒå›ç­”çš„è¿è´¯æ€§ã€‚"""
    else:
        return current_prompt


def init_chat_interface():
    """åˆå§‹åŒ–èŠå¤©ç•Œé¢ï¼Œæ˜¾ç¤ºå†å²æ¶ˆæ¯"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    for msg in st.session_state.messages:
        role = msg["role"]
        content = msg.get("cleaned", msg["content"])  # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹
        
        with st.chat_message(role):
            st.markdown(content)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å«æ€ç»´é“¾
            if role == "assistant" and msg.get("think"):
                with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆå†å²å¯¹è¯ï¼‰"):
                    for think_content in msg["think"]:
                        st.markdown(f'<span style="color: #808080">{think_content.strip()}</span>',
                                  unsafe_allow_html=True)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”æœ‰å‚è€ƒä¾æ®ï¼ˆéœ€è¦ä¿æŒåŸæœ‰å‚è€ƒä¾æ®é€»è¾‘ï¼‰
            if role == "assistant" and "reference_nodes" in msg:
                show_reference_details(msg["reference_nodes"])


def show_reference_details(nodes):
    """æ˜¾ç¤ºå‚è€ƒä¾æ®è¯¦æƒ…"""
    with st.expander("æŸ¥çœ‹æ”¯æŒä¾æ®"):
        for idx, node in enumerate(nodes, 1):
            meta = node.node.metadata
            st.markdown(f"**[{idx}] {meta['full_title']}**")
            st.caption(f"æ¥æºæ–‡ä»¶ï¼š{meta['source_file']} | æ³•å¾‹åç§°ï¼š{meta['law_name']}")
            st.markdown(f"ç›¸å…³åº¦ï¼š`{node.score:.4f}`")
            st.info(f"{node.node.text}")


def synthesize_with_retries(synthesizer, prompt: str, nodes: List, retries: int = 3, initial_delay: float = 2.0):
    """å¯¹ response_synthesizer.synthesize æ·»åŠ æœ‰é™é‡è¯•å’ŒæŒ‡æ•°é€€é¿ã€‚

    å‚æ•°:
        synthesizer: response_synthesizer å®ä¾‹
        prompt: ç”¨æˆ·è¾“å…¥
        nodes: ç”¨äºåˆæˆçš„èŠ‚ç‚¹åˆ—è¡¨
        retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        initial_delay: åˆå§‹ç­‰å¾…ç§’æ•°ï¼Œåç»­æŒ‰ 2^n æŒ‡æ•°å¢é•¿
    è¿”å›:
        åˆæˆå™¨è¿”å›çš„å¯¹è±¡ï¼ˆä¸åŸ synthesize è¿”å›ç›¸åŒï¼‰
    æŠ›å‡º:
        æœ€åä¸€æ¬¡å¼‚å¸¸ï¼ˆè‹¥å…¨éƒ¨é‡è¯•å¤±è´¥ï¼‰
    """
    last_exc = None
    for attempt in range(1, retries + 1):
        try:
            return synthesizer.synthesize(prompt, nodes=nodes)
        except Exception as e:
            last_exc = e
            print(f"[synthesize_with_retries] å°è¯• {attempt}/{retries} å¤±è´¥: {e}")
            traceback.print_exc()
            if attempt == retries:
                # é‡è¯•å®Œæ¯•ï¼Œé‡å¤æŠ›å‡ºæœ€åçš„å¼‚å¸¸
                raise
            # æŒ‡æ•°é€€é¿
            wait = initial_delay * (2 ** (attempt - 1))
            print(f"[synthesize_with_retries] ç­‰å¾… {wait}s åé‡è¯•...")
            time.sleep(wait)


def is_legal_related(question: str, llm) -> bool:
    """åˆ¤æ–­ç”¨æˆ·é—®é¢˜æ˜¯å¦ä¸æ³•å¾‹ç›¸å…³
    
    å‚æ•°:
        question: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        llm: LLMå®ä¾‹ï¼Œç”¨äºåˆ¤æ–­
    è¿”å›:
        True: ä¸æ³•å¾‹ç›¸å…³ï¼Œéœ€è¦å¯ç”¨æ£€ç´¢
        False: ä¸æ³•å¾‹æ— å…³ï¼Œç›´æ¥ä½¿ç”¨å¯¹è¯æ¨¡å‹å›ç­”
    """
    try:
        # æ„å»ºåˆ¤æ–­æç¤º
        judgment_prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹é—®é¢˜æ˜¯å¦ä¸æ³•å¾‹ã€æ³•è§„ã€æ³•å¾‹å’¨è¯¢ã€æ³•å¾‹é—®é¢˜ç›¸å…³ã€‚

é—®é¢˜ï¼š{question}

è¯·åªå›ç­”"æ˜¯"æˆ–"å¦"ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

å¦‚æœé—®é¢˜æ¶‰åŠï¼š
- æ³•å¾‹æ³•è§„ã€æ³•å¾‹æ¡æ–‡ã€æ³•å¾‹æ¡æ¬¾
- æ³•å¾‹å’¨è¯¢ã€æ³•å¾‹é—®é¢˜ã€æ³•å¾‹çº çº·
- åˆåŒã€åè®®ã€æ³•å¾‹æ–‡ä»¶
- è¯‰è®¼ã€ä»²è£ã€æ³•å¾‹ç¨‹åº
- æ³•å¾‹æƒåˆ©ã€æ³•å¾‹ä¹‰åŠ¡ã€æ³•å¾‹è´£ä»»
- ä»»ä½•éœ€è¦å‚è€ƒæ³•å¾‹æ¡æ–‡æ¥å›ç­”çš„é—®é¢˜

è¯·å›ç­”"æ˜¯"ã€‚

å¦‚æœé—®é¢˜æ˜¯ä¸€èˆ¬æ€§å¯¹è¯ã€é—²èŠã€éæ³•å¾‹ç›¸å…³çš„æŠ€æœ¯é—®é¢˜ã€ç”Ÿæ´»å¸¸è¯†ç­‰ï¼Œè¯·å›ç­”"å¦"ã€‚

å›ç­”ï¼š"""
        
        # è°ƒç”¨LLMè¿›è¡Œåˆ¤æ–­
        response = llm.complete(judgment_prompt)
        result = response.text.strip().lower()
        
        # è§£æç»“æœ
        if "æ˜¯" in result or "yes" in result or "true" in result or "1" in result:
            print(f"[is_legal_related] åˆ¤æ–­ç»“æœï¼šä¸æ³•å¾‹ç›¸å…³")
            return True
        else:
            print(f"[is_legal_related] åˆ¤æ–­ç»“æœï¼šä¸æ³•å¾‹æ— å…³")
            return False
            
    except Exception as e:
        # å¦‚æœåˆ¤æ–­å¤±è´¥ï¼Œé»˜è®¤å¯ç”¨æ£€ç´¢ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        print(f"[is_legal_related] åˆ¤æ–­å¤±è´¥: {e}ï¼Œé»˜è®¤å¯ç”¨æ£€ç´¢")
        traceback.print_exc()
        return True


def handle_chat_message_streaming(
    prompt: str,
    retriever,
    response_synthesizer,
    llm_choice: str,
    min_rerank_score: float,
    try_auto_switch_llm_func
):
    """å¤„ç†ç”¨æˆ·èŠå¤©æ¶ˆæ¯ï¼ˆæµå¼è¾“å‡ºï¼‰
    
    å‚æ•°:
        prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        retriever: æ£€ç´¢å™¨å®ä¾‹
        response_synthesizer: å“åº”åˆæˆå™¨å®ä¾‹
        llm_choice: å½“å‰é€‰æ‹©çš„LLMæ¨¡å‹
        min_rerank_score: æœ€å°é‡æ’åºåˆ†æ•°é˜ˆå€¼
        try_auto_switch_llm_func: è‡ªåŠ¨åˆ‡æ¢LLMçš„å‡½æ•°
    è¿”å›:
        tuple: (response_text, filtered_nodes, used_rank)
    """
    start_time = time.time()
    used_rank = False
    
    # é¦–å…ˆåˆ¤æ–­é—®é¢˜æ˜¯å¦ä¸æ³•å¾‹ç›¸å…³
    llm = st.session_state.get("llm")
    if llm is None:
        st.error("âŒ LLMæœªåˆå§‹åŒ–")
        st.stop()
    
    # è·å–å†å²æ¶ˆæ¯ç”¨äºæ„å»ºä¸Šä¸‹æ–‡
    history_messages = st.session_state.get("messages", [])
    
    is_legal = is_legal_related(prompt, llm)
    
    if not is_legal:
        # ä¸æ³•å¾‹æ— å…³ï¼Œç›´æ¥ä½¿ç”¨å¯¹è¯æ¨¡å‹å›ç­”ï¼ˆæµå¼ï¼‰
        st.info("ğŸ’¬ æ£€æµ‹åˆ°é—®é¢˜ä¸æ³•å¾‹æ— å…³ï¼Œä½¿ç”¨å¯¹è¯æ¨¡å¼å›ç­”")
        
        # æ„å»ºåŒ…å«å†å²ä¸Šä¸‹æ–‡çš„æç¤º
        prompt_with_history = build_prompt_with_history(prompt, history_messages)
        
        # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å®¹å™¨
        with st.chat_message("assistant"):
            try:
                # ä½¿ç”¨æµå¼è¾“å‡º
                response_text = ""
                message_placeholder = st.empty()
                try:
                    for token in llm.stream_complete(prompt_with_history):
                        if hasattr(token, 'delta'):
                            response_text += token.delta
                        else:
                            response_text += str(token)
                        message_placeholder.markdown(response_text + "â–Œ")
                    message_placeholder.markdown(response_text)
                except (AttributeError, TypeError):
                    # å¦‚æœä¸æ”¯æŒæµå¼ï¼Œä½¿ç”¨éæµå¼æ–¹æ³•
                    response = llm.complete(prompt_with_history)
                    response_text = response.text
                    # æ¨¡æ‹Ÿæµå¼æ•ˆæœ
                    for i in range(0, len(response_text), 5):
                        chunk = response_text[:i+5]
                        message_placeholder.markdown(chunk + "â–Œ")
                        time.sleep(0.01)
                    message_placeholder.markdown(response_text)
                filtered_nodes = []  # éæ³•å¾‹é—®é¢˜æ²¡æœ‰å‚è€ƒä¾æ®
            except Exception as e:
                print(f"[handle_chat_message_streaming] ç›´æ¥å¯¹è¯æ¨¡å¼å¤±è´¥: {e}")
                traceback.print_exc()
                response_text = f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}"
                st.markdown(response_text)
                filtered_nodes = []
    else:
        # ä¸æ³•å¾‹ç›¸å…³ï¼Œå¯ç”¨æ£€ç´¢æµç¨‹
        st.info("âš–ï¸ æ£€æµ‹åˆ°é—®é¢˜ä¸æ³•å¾‹ç›¸å…³ï¼Œå¯ç”¨æ³•å¾‹æ£€ç´¢æ¨¡å¼")
        
        # æ£€ç´¢æµç¨‹
        initial_nodes = retriever.retrieve(prompt)
        
        # ä½¿ç”¨ä¼šè¯çŠ¶æ€ä¸­çš„ rerankerï¼ˆä»…åœ¨å¯ç”¨ä¸”å·²åŠ è½½æ—¶ä½¿ç”¨ï¼‰
        reranker = st.session_state.reranker
        enable_rank = st.session_state.get("enable_rank_model", False)
        
        if enable_rank and reranker is not None and hasattr(reranker, 'is_loaded') and reranker.is_loaded():
            try:
                reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=prompt)
                # è¿‡æ»¤èŠ‚ç‚¹ï¼ˆæŒ‰åˆ†æ•°è¿‡æ»¤ï¼‰
                filtered_nodes = [node for node in reranked_nodes if node.score > min_rerank_score]
                # å¯ç”¨rankæ¨¡å‹æ—¶ï¼Œå–å‰ RERANK_TOP_K æ¡
                filtered_nodes = filtered_nodes[:Config.RERANK_TOP_K]
                st.success("âœ… å·²ä½¿ç”¨é‡æ’åºåŠŸèƒ½")
                used_rank = True
            except Exception as e:
                st.warning(f"âš ï¸ é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€æ£€ç´¢ç»“æœ")
                # å›é€€åˆ°æŒ‰æ£€ç´¢ç›¸ä¼¼åº¦æ’åºçš„å‰ TOP_K æ¡
                filtered_nodes = initial_nodes[:Config.TOP_K]
                used_rank = False
        else:
            # å¦‚æœæ²¡æœ‰å¯ç”¨é‡æ’åºæ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨åˆå§‹èŠ‚ç‚¹ï¼Œå–å‰ TOP_K æ¡
            st.info("âš ï¸ Rankæ¨¡å‹æœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€æ£€ç´¢ç»“æœ")
            filtered_nodes = initial_nodes[:Config.TOP_K]  # ä½¿ç”¨æ£€ç´¢å¾—åˆ°çš„å‰ TOP_K æ¡
            used_rank = False
        
        # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å®¹å™¨ï¼ˆæµå¼è¾“å‡ºä¼šåœ¨è¿™é‡Œæ˜¾ç¤ºï¼‰
        with st.chat_message("assistant"):
            if not filtered_nodes:
                response_text = "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼Œè¯·å°è¯•è°ƒæ•´é—®é¢˜æè¿°æˆ–å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆã€‚"
                st.markdown(response_text)
            else:
                # æ„é€ å¸¦æœ‰æ³•å¾‹RAGæç¤ºè¯çš„ç³»ç»Ÿæç¤º
                legal_prompt_text = ""
                try:
                    legal_prompt_path = Path(Config.LEGAL_CHAT_PROMPT_PATH)
                    if legal_prompt_path.exists():
                        legal_prompt_text = legal_prompt_path.read_text(encoding="utf-8")
                except Exception as e:
                    print(f"[handle_chat_message_streaming] è¯»å–æ³•å¾‹æç¤ºè¯æ¨¡ç‰ˆå¤±è´¥: {e}")

                # æ„å»ºåŒ…å«å†å²ä¸Šä¸‹æ–‡çš„æç¤º
                history_context = build_conversation_context(history_messages)
                
                if legal_prompt_text:
                    if history_context:
                        full_prompt = f"{legal_prompt_text}\n\nã€å¯¹è¯å†å²ã€‘\n{history_context}\n\nã€å½“å‰é—®é¢˜ã€‘\n{prompt}"
                    else:
                        full_prompt = f"{legal_prompt_text}\n\nç”¨æˆ·é—®é¢˜ï¼š{prompt}"
                else:
                    if history_context:
                        full_prompt = f"ã€å¯¹è¯å†å²ã€‘\n{history_context}\n\nã€å½“å‰é—®é¢˜ã€‘\n{prompt}"
                    else:
                        full_prompt = prompt

                # ç”Ÿæˆå›ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰
                try:
                    # ä½¿ç”¨æµå¼åˆæˆå“åº”
                    response_text = ""
                    message_placeholder = st.empty()
                    
                    # å°è¯•ä½¿ç”¨æµå¼æ–¹æ³•
                    try:
                        # ä½¿ç”¨ response_synthesizer çš„æµå¼æ–¹æ³•
                        response_gen = response_synthesizer.astream_response(full_prompt, nodes=filtered_nodes)
                        for token in response_gen.response_gen:
                            response_text += token
                            message_placeholder.markdown(response_text + "â–Œ")
                        message_placeholder.markdown(response_text)
                    except (AttributeError, TypeError):
                        # å¦‚æœä¸æ”¯æŒæµå¼ï¼Œå›é€€åˆ°éæµå¼æ–¹æ³•
                        print("[handle_chat_message_streaming] å“åº”åˆæˆå™¨ä¸æ”¯æŒæµå¼ï¼Œä½¿ç”¨éæµå¼æ–¹æ³•")
                        response = synthesize_with_retries(response_synthesizer, full_prompt, filtered_nodes, retries=3)
                        response_text = response.response
                        # æ¨¡æ‹Ÿæµå¼è¾“å‡ºæ•ˆæœ
                        for i in range(0, len(response_text), 5):
                            chunk = response_text[:i+5]
                            message_placeholder.markdown(chunk + "â–Œ")
                            time.sleep(0.01)
                        message_placeholder.markdown(response_text)
                except Exception as e:
                    # æ‰“å°è¯¦ç»†è·Ÿè¸ªä»¥ä¾¿è°ƒè¯•
                    print("[handle_chat_message_streaming] response_synthesizer ç”Ÿæˆå¤±è´¥ï¼Œè¿›å…¥å›é€€é€»è¾‘:")
                    traceback.print_exc()
                    # å‘ç”¨æˆ·æ˜¾ç¤ºå‹å¥½æç¤º
                    st.error("âš ï¸ åç«¯æ¨¡å‹æœåŠ¡å¼‚å¸¸ï¼Œæ­£åœ¨å°è¯•åˆ‡æ¢å¤‡ç”¨æ¨¡å‹æˆ–å›é€€ä¸ºä¸´æ—¶ç»“æœã€‚")

                    # ä¼˜å…ˆå°è¯•è‡ªåŠ¨åˆ‡æ¢åˆ°å…¶å®ƒå¯ç”¨æ¨¡å‹å¹¶é‡è¯•ä¸€æ¬¡
                    switched = False
                    try:
                        switched = try_auto_switch_llm_func(st.session_state.get('current_llm_choice', llm_choice))
                    except Exception as e_switch:
                        print(f"[handle_chat_message_streaming] è‡ªåŠ¨åˆ‡æ¢æ¨¡å‹è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e_switch}")

                    if switched:
                        try:
                            # ä½¿ç”¨æ–°çš„ LLM é‡æ–°åˆ›å»ºåˆæˆå™¨å¹¶é‡è¯•ï¼ˆæµå¼ï¼‰
                            response_synthesizer = get_response_synthesizer(verbose=True)
                            response_text = ""
                            message_placeholder = st.empty()
                            try:
                                response_gen = response_synthesizer.astream_response(prompt, nodes=filtered_nodes)
                                for token in response_gen.response_gen:
                                    response_text += token
                                    message_placeholder.markdown(response_text + "â–Œ")
                                message_placeholder.markdown(response_text)
                            except (AttributeError, TypeError):
                                response = synthesize_with_retries(response_synthesizer, prompt, filtered_nodes, retries=2)
                                response_text = response.response
                                for i in range(0, len(response_text), 5):
                                    chunk = response_text[:i+5]
                                    message_placeholder.markdown(chunk + "â–Œ")
                                    time.sleep(0.01)
                                message_placeholder.markdown(response_text)
                            # å¦‚æœæˆåŠŸåˆ™è·³è¿‡åç»­å›é€€é€»è¾‘
                        except Exception as e2:
                            print("[handle_chat_message_streaming] åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹åé‡è¯•ä»å¤±è´¥:", e2)
                            traceback.print_exc()
                            switched = False

                    if not switched:
                        # å°†å‰3æ¡æ£€ç´¢åˆ°çš„æ–‡æ¡£æ‹¼æ¥ä¸ºä¸´æ—¶å†…å®¹
                        concatenated = "\n\n".join([n.node.text for n in filtered_nodes[:3]])

                        # å°è¯•ä½¿ç”¨æœ¬åœ°å°æ¨¡å‹åšå¿«é€Ÿæ‘˜è¦ï¼ˆå¦‚æœé…ç½®å¹¶å­˜åœ¨æœ¬åœ°æ¨¡å‹ï¼‰
                        summary_text = None
                        try:
                            local_cfg = LLM_CONFIGS.get("local")
                            if local_cfg:
                                local_model_path = local_cfg.get("model")
                                if local_model_path and Path(local_model_path).exists():
                                    try:
                                        hf_llm = HuggingFaceLLM(model_name=str(local_model_path), temperature=0.2, max_length=256)
                                        # ä½¿ç”¨hf_llmè¿›è¡Œå¿«é€Ÿæ‘˜è¦
                                        summary_text = hf_llm.predict(f"è¯·ç®€è¦æ€»ç»“ä»¥ä¸‹æ³•å¾‹æ¡æ–‡è¦ç‚¹ï¼š\n\n{concatenated}\n\næ€»ç»“ï¼š")
                                    except Exception as e_local:
                                        print(f"[fallback] æœ¬åœ°æ¨¡å‹æ‘˜è¦å¤±è´¥: {e_local}")
                        except Exception as e_cfg:
                            print(f"[fallback] æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e_cfg}")

                        if summary_text:
                            cleaned_response = summary_text
                            response_text = f"âš ï¸ åç«¯æœåŠ¡å¼‚å¸¸ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆçš„ä¸´æ—¶æ‘˜è¦ï¼š\n\n{summary_text}"
                            message_placeholder = st.empty()
                            message_placeholder.markdown(response_text)
                        else:
                            # å›é€€åˆ°æ‹¼æ¥çš„åŸæ–‡
                            cleaned_response = concatenated
                            response_text = f"âš ï¸ åç«¯æ¨¡å‹æœåŠ¡å¼‚å¸¸ï¼š{e}\n\nç›¸å…³æ¡æ–‡ï¼ˆä¸´æ—¶ç»“æœï¼‰ï¼š\n{concatenated}"
                            message_placeholder = st.empty()
                            message_placeholder.markdown(response_text)
    
    return response_text, filtered_nodes, used_rank


def display_chat_response(response_text: str, filtered_nodes: List, used_rank: bool):
    """æ˜¾ç¤ºèŠå¤©å“åº”ï¼ˆæµå¼è¾“å‡ºå·²åœ¨ handle_chat_message_streaming ä¸­å®Œæˆï¼Œè¿™é‡Œåªå¤„ç†åç»­æ˜¾ç¤ºï¼‰
    
    å‚æ•°:
        response_text: å“åº”æ–‡æœ¬
        filtered_nodes: è¿‡æ»¤åçš„èŠ‚ç‚¹åˆ—è¡¨
        used_rank: æ˜¯å¦ä½¿ç”¨äº†Rankæ¨¡å‹
    """
    # æå–æ€ç»´é“¾å†…å®¹å¹¶æ¸…ç†å“åº”æ–‡æœ¬
    think_contents = re.findall(r'<think>(.*?)</think>', response_text, re.DOTALL)
    cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()
    
    # æµå¼è¾“å‡ºå·²åœ¨ handle_chat_message_streaming ä¸­å®Œæˆï¼Œè¿™é‡Œåªæ˜¾ç¤ºé™„åŠ å†…å®¹
    # å¦‚æœæœ‰æ€ç»´é“¾å†…å®¹åˆ™æ˜¾ç¤º
    if think_contents:
        with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
            for content in think_contents:
                st.markdown(f'<span style="color: #808080">{content.strip()}</span>', 
                          unsafe_allow_html=True)
    
    # ä»…åœ¨æœ‰å‚è€ƒä¾æ®æ—¶æ˜¾ç¤ºï¼ˆæ³•å¾‹ç›¸å…³é—®é¢˜æ‰æœ‰å‚è€ƒä¾æ®ï¼‰
    if filtered_nodes:
        # å±•ç¤ºæ•°é‡ä¸æ£€ç´¢/é‡æ’åºè®¾ç½®è”åŠ¨ï¼š
        # - å¯ç”¨å¹¶æˆåŠŸä½¿ç”¨ Rank æ—¶ï¼šå±•ç¤º RERANK_TOP_K æ¡ï¼ˆå·²åœ¨å‰é¢å¤„ç†æ—¶æˆªæ–­ï¼‰
        # - æœªå¯ç”¨ Rank æ—¶ï¼šå±•ç¤º TOP_K æ¡ï¼ˆå·²åœ¨å‰é¢å¤„ç†æ—¶æˆªæ–­ï¼‰
        # filtered_nodes å·²ç»æŒ‰ç…§ç›¸åº”æ•°é‡æˆªæ–­äº†ï¼Œç›´æ¥æ˜¾ç¤ºå…¨éƒ¨
        ref_k = len(filtered_nodes)
        show_reference_details(filtered_nodes)
    
    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²ï¼ˆéœ€è¦å­˜å‚¨åŸå§‹å“åº”ï¼‰
    if filtered_nodes:
        ref_k = len(filtered_nodes)
    else:
        ref_k = 0
    
    st.session_state.messages.append({
        "role": "assistant",
        "content": response_text,  # ä¿ç•™åŸå§‹å“åº”
        "cleaned": cleaned_response,  # å­˜å‚¨æ¸…ç†åçš„æ–‡æœ¬
        "think": think_contents,  # å­˜å‚¨æ€ç»´é“¾å†…å®¹
        "reference_nodes": filtered_nodes[:ref_k] if filtered_nodes else []  # å­˜å‚¨å‚è€ƒèŠ‚ç‚¹
    })

