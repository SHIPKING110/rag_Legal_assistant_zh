# -*- coding: utf-8 -*-
import json
import time
import os
from pathlib import Path
from typing import List, Dict, Optional
import re
import chromadb
import traceback
from dotenv import load_dotenv, set_key

import streamlit as st
import psutil
import torch
import requests
from llama_index.core import VectorStoreIndex, StorageContext, Settings, get_response_synthesizer
from llama_index.core.schema import TextNode, NodeWithScore
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core import QueryBundle
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import PromptTemplate
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.llms.openai_like import OpenAILike

# å¯¼å…¥é…ç½®å’Œæ¨¡å‹é…ç½®
from utils import Config, LLM_CONFIGS, DEEPSEEK_MODELS, GLM_MODELS, QWEN_MODELS

# å¯¼å…¥èŠå¤©æ¨¡å—
from webui.chat_model import (
    init_chat_interface,
    show_reference_details,
    handle_chat_message_streaming,
    display_chat_response
)

# å¯¼å…¥ä¼šè¯å†å²ç®¡ç†æ¨¡å—
from webui.chat_history import (
    ChatHistoryManager,
    init_session_state_for_chat_history,
    render_chat_history_sidebar,
    render_new_session_button
)

# å¯¼å…¥æ³•å¾‹æ£€ç´¢æ¨¡å—
from webui.legal_search import render_legal_search_page

# å¯¼å…¥è®¤è¯æ¨¡å—
from webui.auth import check_authentication, render_login_page, render_user_info_sidebar, is_admin

# å¯¼å…¥æ¸¸å®¢é¡µé¢
from webui.guest_page import render_guest_page

# å¯¼å…¥ç®¡ç†å‘˜é¢æ¿
from webui.admin import render_admin_panel

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

# æ–‡æ¡£è¯´æ˜æ–‡ä»¶è·¯å¾„
DOC_DESCRIPTION_DIR = Path(__file__).parent / 'webui' / 'doc_description'
DOC_PAGES = {
    "åŠŸèƒ½ä»‹ç»": DOC_DESCRIPTION_DIR / "features.md",
    "æ¨¡å‹åŠŸèƒ½": DOC_DESCRIPTION_DIR / "models.md",
    "APIç”³è¯·": DOC_DESCRIPTION_DIR / "api.md",
}
DOC_PLACEHOLDER = "è¯·é€‰æ‹©æ–‡æ¡£"

# è‡ªåŠ¨åŠ è½½.envæ–‡ä»¶
load_dotenv(dotenv_path=Path(__file__).parent / '.env', override=True)

# ================== Streamlité¡µé¢é…ç½® ==================
st.set_page_config(
    page_title="æ™ºèƒ½æ³•å¾‹å’¨è¯¢åŠ©æ‰‹",
    page_icon="âš–ï¸",
    layout="centered",
    initial_sidebar_state="auto"
)

def disable_streamlit_watcher():
    """æ›´å®‰å…¨çš„æ–¹å¼ç¦ç”¨Streamlitæ–‡ä»¶ç›‘è§†å™¨"""
    try:
        from streamlit import runtime
        if runtime.exists():
            instance = runtime.get_instance()
            def _on_script_changed(_):
                return
            if hasattr(instance, '_on_script_changed'):
                instance._on_script_changed = _on_script_changed
    except Exception as e:
        print(f"ç¦ç”¨æ–‡ä»¶ç›‘è§†å™¨æ—¶å‡ºç°è­¦å‘Š: {e}")


def load_doc_markdown(doc_key: str) -> str:
    """è¯»å–æŒ‡å®šæ–‡æ¡£è¯´æ˜å†…å®¹"""
    path = DOC_PAGES.get(doc_key)
    if path is None:
        return f"âš ï¸ æœªæ‰¾åˆ° {doc_key} å¯¹åº”çš„æ–‡æ¡£é…ç½®ã€‚"
    try:
        return path.read_text(encoding='utf-8')
    except FileNotFoundError:
        return f"âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶ï¼š{path}"
    except Exception as exc:
        return f"âš ï¸ è¯»å–æ–‡æ¡£è¯´æ˜æ–‡ä»¶å¤±è´¥ï¼š{exc}"


def show_documentation_page():
    """å±•ç¤ºæ–‡æ¡£è¯´æ˜é¡µé¢"""
    stored_key = st.session_state.get("doc_category")
    if stored_key not in DOC_PAGES:
        stored_key = next(iter(DOC_PAGES))
        st.session_state.doc_category = stored_key
    doc_key = stored_key
    st.subheader(f"ğŸ“˜ æ–‡æ¡£è¯´æ˜ Â· {doc_key}")
    st.markdown(load_doc_markdown(doc_key))
    st.divider()
    if st.button("è¿”å›èŠå¤©å¯¹è¯", use_container_width=True):
        st.session_state.show_docs = False
        st.session_state.doc_category = DOC_PLACEHOLDER
        st.rerun()

# ================== è®¾å¤‡æ£€æµ‹å’Œå†…å­˜å·¥å…· ==================
def detect_device():
    """æ£€æµ‹è®¾å¤‡æ˜¯å¦æ”¯æŒGPUï¼Œè¿”å›è®¾å¤‡ç±»å‹"""
    if torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
        return device, f"GPU ({device_name})"
    else:
        return "cpu", "CPU"

def get_available_memory_gb():
    """è·å–ç³»ç»Ÿå¯ç”¨å†…å­˜ï¼ˆGBï¼‰"""
    memory = psutil.virtual_memory()
    available_gb = memory.available / (1024 ** 3)
    return available_gb

def check_rank_model_memory():
    """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜åŠ è½½rankæ¨¡å‹"""
    available_memory = get_available_memory_gb()
    required_memory = Config.RERANK_MODEL_MIN_MEMORY_GB
    
    return available_memory >= required_memory, available_memory, required_memory

# ================== ä¿®å¤åçš„è‡ªå®šä¹‰é‡æ’åºå™¨ ==================
class SimpleQwenReranker(BaseNodePostprocessor):
    # ä½¿ç”¨ Pydantic å­—æ®µå®šä¹‰
    model_path: str
    top_n: int = 3
    device: str = "cpu"
    auto_load: bool = False
    
    def __init__(self, model_path: str, top_n: int = 3, device: str = "cpu", auto_load: bool = False):
        # ä½¿ç”¨ Pydantic çš„æ–¹å¼åˆå§‹åŒ–ï¼ˆæ‰€æœ‰å‚æ•°éƒ½å¿…é¡»ä¼ é€’ç»™super().__init__ï¼‰
        super().__init__(model_path=model_path, top_n=top_n, device=device, auto_load=auto_load)
        
        # åˆå§‹åŒ–å†…éƒ¨çŠ¶æ€
        self._is_loaded = False
        self._model = None
        
        # ä»…å½“auto_loadä¸ºTrueæ—¶æ‰åŠ è½½æ¨¡å‹
        if auto_load:
            self._try_load_model()
    
    def _try_load_model(self):
        """å°è¯•åŠ è½½æ¨¡å‹ï¼Œä½†ä¸æŠ›å‡ºå¼‚å¸¸"""
        try:
            from sentence_transformers import CrossEncoder
            self._model = CrossEncoder(
                self.model_path, 
                trust_remote_code=True, 
                local_files_only=True,
                device=self.device
            )
            
            # ä¿®å¤ï¼šè®¾ç½®å¡«å……ä»¤ç‰Œ
            if hasattr(self._model, 'tokenizer') and self._model.tokenizer.pad_token is None:
                self._model.tokenizer.pad_token = self._model.tokenizer.eos_token
            
            self._is_loaded = True
            print(f"âœ… Qwen3-Reranker åŠ è½½æˆåŠŸ (è®¾å¤‡: {self.device}): {self.model_path}")
            
        except Exception as e:
            print(f"âŒ é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self._is_loaded = False
    
    def is_loaded(self):
        return self._is_loaded
    
    def load_model(self):
        """ä¸»åŠ¨åŠ è½½æ¨¡å‹"""
        if not self._is_loaded:
            self._try_load_model()
        return self._is_loaded
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜"""
        if self._model is not None:
            del self._model
            self._model = None
            self._is_loaded = False
            print("âœ… Rankæ¨¡å‹å·²å¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾")
    
    def _postprocess_nodes(self, nodes: List[NodeWithScore], query_bundle: QueryBundle):
        if not nodes or not self._is_loaded or self._model is None:
            return nodes[:self.top_n] if nodes else []
        
        try:
            # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
            query_doc_pairs = []
            for node in nodes:
                query_doc_pairs.append([query_bundle.query_str, node.node.get_content()])
            
            # é€ä¸ªå¤„ç†ï¼Œé¿å…æ‰¹é‡å¡«å……é—®é¢˜
            scores = []
            for pair in query_doc_pairs:
                score = self._model.predict([pair])
                scores.append(float(score[0]))
            
            # å°†åˆ†æ•°æ·»åŠ åˆ°èŠ‚ç‚¹
            for node, score in zip(nodes, scores):
                node.score = score
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰top_nä¸ª
            sorted_nodes = sorted(nodes, key=lambda x: x.score, reverse=True)
            return sorted_nodes[:self.top_n]
            
        except Exception as e:
            print(f"é‡æ’åºå¤±è´¥: {e}")
            return nodes[:self.top_n]

# ================== ç¼“å­˜èµ„æºåˆå§‹åŒ– ==================
@st.cache_resource(show_spinner="åˆå§‹åŒ–æ¨¡å‹ä¸­...")
def init_models(llm_choice="deepseek", api_key=None, llm_sub_choice=None):
    # æ£€æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    embed_model_path = Path(Config.EMBED_MODEL_PATH)
    if not embed_model_path.exists():
        st.error(f"âŒ åµŒå…¥æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {Config.EMBED_MODEL_PATH}")
        st.info("è¯·ç¡®ä¿æ¨¡å‹å·²æ­£ç¡®ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„")
        st.stop()
    
    embed_model = HuggingFaceEmbedding(
        model_name=str(embed_model_path),
    )
    
    # æ£€æŸ¥é‡æ’åºæ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆé»˜è®¤ä¸åŠ è½½ï¼‰
    rerank_model_path = Path(Config.RERANK_MODEL_PATH)
    if not rerank_model_path.exists():
        st.warning(f"âš ï¸ é‡æ’åºæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {Config.RERANK_MODEL_PATH}")
        st.info("rankæ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨")
        reranker = None
    else:
        try:
            # æ£€æµ‹è®¾å¤‡
            device, device_name = detect_device()
            
            # åˆ›å»ºé‡æ’åºå™¨å®ä¾‹ï¼Œä½†ä¸è‡ªåŠ¨åŠ è½½ï¼ˆauto_load=Falseï¼‰
            reranker = SimpleQwenReranker(
                model_path=str(rerank_model_path),
                top_n=Config.RERANK_TOP_K,
                device=device,
                auto_load=False  # é»˜è®¤ä¸åŠ è½½
            )
            print(f"âœ… Rankæ¨¡å‹å·²åˆå§‹åŒ–ï¼ˆæœªåŠ è½½ï¼‰, æ£€æµ‹åˆ°è®¾å¤‡: {device_name}")
                
        except Exception as e:
            # æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            import traceback
            error_details = traceback.format_exc()
            st.error(f"âŒ é‡æ’åºæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            st.info("å°†ç¦ç”¨é‡æ’åºåŠŸèƒ½ï¼Œä»…ä½¿ç”¨åŸºç¡€æ£€ç´¢")
            reranker = None
    
    # åŸºç¡€é…ç½®
    config = dict(LLM_CONFIGS[llm_choice])
    
    # æ ¹æ®å­æ¨¡å‹é€‰æ‹©è¦†ç›–é…ç½®ï¼ˆä»… deepseek / glm / qwen æ”¯æŒï¼‰
    if llm_choice == "deepseek" and llm_sub_choice and llm_sub_choice in DEEPSEEK_MODELS:
        config.update(DEEPSEEK_MODELS[llm_sub_choice])
    elif llm_choice == "glm" and llm_sub_choice and llm_sub_choice in GLM_MODELS:
        config.update(GLM_MODELS[llm_sub_choice])
    elif llm_choice == "qwen" and llm_sub_choice and llm_sub_choice in QWEN_MODELS:
        config.update(QWEN_MODELS[llm_sub_choice])
    
    if llm_choice == "deepseek":
        if not api_key:
            st.error("âŒ è¯·æä¾›DeepSeek API Key")
            Settings.embed_model = embed_model
            return embed_model, None, reranker, llm_choice
        
        llm = OpenAILike(
            model=config["model"],
            api_base=config["api_base"],
            api_key=api_key,
            context_window=config["context_window"],
            is_chat_model=True,
            is_function_calling_model=False,
            max_tokens=config["max_tokens"],
            temperature=config["temperature"],
            top_p=config["top_p"]
        )
    elif llm_choice == "glm":
        if not api_key:
            st.error("âŒ è¯·æä¾›GLM API Key")
            Settings.embed_model = embed_model
            return embed_model, None, reranker, llm_choice
            
        llm = OpenAILike(
            model=config["model"],
            api_base=config["api_base"],
            api_key=api_key,
            context_window=config["context_window"],
            is_chat_model=True,
            is_function_calling_model=False,
            max_tokens=config["max_tokens"],
            temperature=config["temperature"],
            top_p=config["top_p"]
        )
    elif llm_choice == "qwen":
        if not api_key:
            st.error("âŒ è¯·æä¾›Qwen API Key")
            Settings.embed_model = embed_model
            return embed_model, None, reranker, llm_choice
        
        llm = OpenAILike(
            model=config["model"],
            api_base=config["api_base"],
            api_key=api_key,
            context_window=config["context_window"],
            is_chat_model=True,
            is_function_calling_model=False,
            max_tokens=config["max_tokens"],
            temperature=config["temperature"],
            top_p=config["top_p"]
        )
    else:  # local
        llm = OpenAILike(
            model=config["model"],
            api_base=config["api_base"],
            api_key="fake",
            context_window=config["context_window"],
            is_chat_model=True,
            is_function_calling_model=False,
            max_tokens=config["max_tokens"],
            temperature=config["temperature"],
            top_p=config["top_p"]
        )
    
    Settings.embed_model = embed_model
    Settings.llm = llm
    
    return embed_model, llm, reranker, llm_choice

@st.cache_resource(show_spinner="åŠ è½½çŸ¥è¯†åº“ä¸­...")
def init_vector_store(_nodes):
    chroma_client = chromadb.PersistentClient(path=Config.VECTOR_DB_DIR)
    chroma_collection = chroma_client.get_or_create_collection(
        name=Config.COLLECTION_NAME,
        metadata={"hnsw:space": "cosine"}
    )

    if chroma_collection.count() == 0 and _nodes is not None:
        # æ–°å»ºç´¢å¼•
        storage_context = StorageContext.from_defaults(
            vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
        )
        storage_context.docstore.add_documents(_nodes)  
        index = VectorStoreIndex(
            _nodes,
            storage_context=storage_context,
            show_progress=True
        )
        # åˆ›å»ºpersistç›®å½•
        Path(Config.PERSIST_DIR).mkdir(parents=True, exist_ok=True)
        storage_context.persist(persist_dir=Config.PERSIST_DIR)
        index.storage_context.persist(persist_dir=Config.PERSIST_DIR)
    else:
        # åŠ è½½ç°æœ‰ç´¢å¼•
        storage_context = StorageContext.from_defaults(
            vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
        )
        index = VectorStoreIndex.from_vector_store(
            storage_context.vector_store,
            storage_context=storage_context,
            embed_model=Settings.embed_model
        )
    return index

# ================== æ•°æ®å¤„ç† ==================
def load_and_validate_json_files(data_dir: str) -> List[Dict]:
    """åŠ è½½å¹¶éªŒè¯JSONæ³•å¾‹æ–‡ä»¶"""
    json_files = list(Path(data_dir).glob("*.json"))
    assert json_files, f"æœªæ‰¾åˆ°JSONæ–‡ä»¶äº {data_dir}"
    
    all_data = []
    for json_file in json_files:
        with open(json_file, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                # éªŒè¯æ•°æ®ç»“æ„
                if not isinstance(data, list):
                    raise ValueError(f"æ–‡ä»¶ {json_file.name} æ ¹å…ƒç´ åº”ä¸ºåˆ—è¡¨")
                for item in data:
                    if not isinstance(item, dict):
                        raise ValueError(f"æ–‡ä»¶ {json_file.name} åŒ…å«éå­—å…¸å…ƒç´ ")
                    for k, v in item.items():
                        if not isinstance(v, str):
                            raise ValueError(f"æ–‡ä»¶ {json_file.name} ä¸­é”® '{k}' çš„å€¼ä¸æ˜¯å­—ç¬¦ä¸²")
                all_data.extend({
                    "content": item,
                    "metadata": {"source": json_file.name}
                } for item in data)
            except Exception as e:
                raise RuntimeError(f"åŠ è½½æ–‡ä»¶ {json_file} å¤±è´¥: {str(e)}")
    
    print(f"æˆåŠŸåŠ è½½ {len(all_data)} ä¸ªæ³•å¾‹æ–‡ä»¶æ¡ç›®")
    return all_data

def create_nodes(raw_data: List[Dict]) -> List[TextNode]:
    """æ·»åŠ IDç¨³å®šæ€§ä¿éšœ"""
    nodes = []
    for entry in raw_data:
        law_dict = entry["content"]
        source_file = entry["metadata"]["source"]
        
        for full_title, content in law_dict.items():
            # ç”Ÿæˆç¨³å®šIDï¼ˆé¿å…é‡å¤ï¼‰
            node_id = f"{source_file}::{full_title}"
            
            parts = full_title.split(" ", 1)
            law_name = parts[0] if len(parts) > 0 else "æœªçŸ¥æ³•å¾‹"
            article = parts[1] if len(parts) > 1 else "æœªçŸ¥æ¡æ¬¾"
            
            node = TextNode(
                text=content,
                id_=node_id,  # æ˜¾å¼è®¾ç½®ç¨³å®šID
                metadata={
                    "law_name": law_name,
                    "article": article,
                    "full_title": full_title,
                    "source_file": source_file,
                    "content_type": "legal_article"
                }
            )
            nodes.append(node)
    
    print(f"ç”Ÿæˆ {len(nodes)} ä¸ªæ–‡æœ¬èŠ‚ç‚¹ï¼ˆIDç¤ºä¾‹ï¼š{nodes[0].id_}ï¼‰")
    return nodes

# ================== ç•Œé¢ç»„ä»¶ ==================
def init_sidebar():
    """ä¾§è¾¹æ é…ç½®"""
    with st.sidebar:
        st.header("âš™ï¸ åŠŸèƒ½æ¨¡å—")
        
        if "show_docs" not in st.session_state:
            st.session_state.show_docs = False
        if "doc_category" not in st.session_state:
            st.session_state.doc_category = DOC_PLACEHOLDER
        
        # ========= æ–°å»ºä¼šè¯æŒ‰é’®ï¼ˆæœ€ä¸Šé¢ï¼‰ =========
        render_new_session_button()
        
        # ========= å†å²ä¼šè¯åˆ—è¡¨ =========
        render_chat_history_sidebar()
        
        # é»˜è®¤å€¼åˆå§‹åŒ–ï¼Œé¿å…æœªè¿›å…¥æŠ˜å é¢æ¿æ—¶è¿”å› None
        temperature = 0.3
        top_p = 0.7
        max_tokens = 1024
        top_k = Config.TOP_K
        rerank_top_k = Config.RERANK_TOP_K
        min_rerank_score = 0.4
        api_key = None
        
        # ========= æ¨¡å‹é…ç½® =========
        with st.expander("æ¨¡å‹é…ç½®", expanded=False):
            # LLMé€‰æ‹©ï¼ˆä¿å­˜åˆ° session_stateï¼Œä¾¿äºæŒ‰é’®åˆ‡æ¢ï¼‰
            if 'llm_choice_requested' in st.session_state:
                requested = st.session_state.pop('llm_choice_requested')
                st.session_state.llm_choice_select = requested

            if 'llm_choice_select' not in st.session_state:
                st.session_state.llm_choice_select = 'deepseek'

            llm_choice = st.selectbox(
                "é€‰æ‹©LLMæ¨¡å‹",
                options=["deepseek", "glm", "qwen", "local"],
                format_func=lambda x: {
                    "deepseek": "DeepSeek",
                    "glm": "æ™ºè°±GLM",
                    "qwen": "é˜¿é‡Œäº‘é€šä¹‰Qwen",
                    "local": "æœ¬åœ°æ¨¡å‹"
                }[x],
                key='llm_choice_select'
            )

            env_path = str(Path(__file__).parent / '.env')
            if llm_choice == 'deepseek':
                current = os.environ.get('LLM_API_KEY', '')
                api_input = st.text_input('DeepSeek API Key', value=current, type='password', help='DeepSeek API Keyï¼Œç•™ç©ºåˆ™ä¸èƒ½è°ƒç”¨ DeepSeek')
                if api_input and api_input != current:
                    try:
                        set_key(env_path, 'LLM_API_KEY', api_input)
                        os.environ['LLM_API_KEY'] = api_input
                    except Exception as e:
                        print(f"å†™ .env å¤±è´¥: {e}")
                api_key = os.environ.get('LLM_API_KEY')
            elif llm_choice == 'glm':
                current = os.environ.get('GLM_API_KEY', '')
                api_input = st.text_input('GLM API Key', value=current, type='password', help='GLM API Keyï¼Œç•™ç©ºåˆ™ä¸èƒ½è°ƒç”¨ GLM')
                if api_input and api_input != current:
                    try:
                        set_key(env_path, 'GLM_API_KEY', api_input)
                        os.environ['GLM_API_KEY'] = api_input
                    except Exception as e:
                        print(f"å†™ .env å¤±è´¥: {e}")
                api_key = os.environ.get('GLM_API_KEY')
            elif llm_choice == 'qwen':
                current = os.environ.get('QWEN_API_KEY', '')
                api_input = st.text_input('Qwen API Key', value=current, type='password', help='DashScope API Keyï¼Œç•™ç©ºåˆ™ä¸èƒ½è°ƒç”¨ Qwen')
                if api_input and api_input != current:
                    try:
                        set_key(env_path, 'QWEN_API_KEY', api_input)
                        os.environ['QWEN_API_KEY'] = api_input
                    except Exception as e:
                        print(f"å†™ .env å¤±è´¥: {e}")
                api_key = os.environ.get('QWEN_API_KEY')
            else:
                local_models_dir = Path(__file__).parent / 'model' / 'chat_models'
                local_available = local_models_dir.exists() and any(local_models_dir.iterdir())
                if not local_available:
                    st.warning(f"âš ï¸ æœªæ£€æµ‹åˆ°æœ¬åœ°èŠå¤©æ¨¡å‹äº: {local_models_dir}ã€‚è¯·å…ˆå°†æ¨¡å‹æ”¾å…¥è¯¥ç›®å½•ï¼Œæˆ–åˆ‡æ¢åˆ°äº‘ç«¯æ¨¡å‹ã€‚")
                    col1, col2 = st.columns(2)
                    if col1.button('åˆ‡æ¢åˆ° DeepSeek'):
                        st.session_state.llm_choice_requested = 'deepseek'
                    if col2.button('åˆ‡æ¢åˆ° GLM'):
                        st.session_state.llm_choice_requested = 'glm'

            # å­æ¨¡å‹é€‰æ‹©
            llm_sub_choice = None
            if llm_choice == "deepseek":
                if "llm_sub_choice" not in st.session_state:
                    st.session_state.llm_sub_choice = "deepseek-chat"
                options = list(DEEPSEEK_MODELS.keys())
                llm_sub_choice = st.selectbox(
                    "DeepSeek å­æ¨¡å‹",
                    options=options,
                    index=options.index(st.session_state.llm_sub_choice) if st.session_state.llm_sub_choice in options else 0,
                )
                st.session_state.llm_sub_choice = llm_sub_choice
            elif llm_choice == "glm":
                if "llm_sub_choice" not in st.session_state:
                    st.session_state.llm_sub_choice = "glm-4"
                options = list(GLM_MODELS.keys())
                llm_sub_choice = st.selectbox(
                    "GLM å­æ¨¡å‹",
                    options=options,
                    index=options.index(st.session_state.llm_sub_choice) if st.session_state.llm_sub_choice in options else 0,
                )
                st.session_state.llm_sub_choice = llm_sub_choice
            elif llm_choice == "qwen":
                if "llm_sub_choice" not in st.session_state:
                    st.session_state.llm_sub_choice = "qwen-plus"
                options = list(QWEN_MODELS.keys())
                llm_sub_choice = st.selectbox(
                    "Qwen å­æ¨¡å‹",
                    options=options,
                    index=options.index(st.session_state.llm_sub_choice) if st.session_state.llm_sub_choice in options else 0,
                )
                st.session_state.llm_sub_choice = llm_sub_choice
            else:
                st.session_state.llm_sub_choice = None

        # ========= æ¨¡å‹å‚æ•° =========
        with st.expander("æ¨¡å‹å‚æ•°", expanded=False):
            temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.1)
            top_p = st.slider("Top P", 0.0, 1.0, 0.7, 0.1)
            max_tokens = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 512, 4096, 1024, 128)

        # ========= æ£€ç´¢å‚æ•° =========
        with st.expander("æ£€ç´¢å‚æ•°", expanded=False):
            # ä½¿ç”¨ session_state ä¿å­˜æ»‘å—å€¼ï¼Œé¿å…æ¯æ¬¡é‡æ–°æ¸²æŸ“æ—¶å›åˆ°é»˜è®¤å€¼
            if "top_k_value" not in st.session_state:
                st.session_state.top_k_value = Config.TOP_K
            if "rerank_top_k_value" not in st.session_state:
                st.session_state.rerank_top_k_value = Config.RERANK_TOP_K
            
            top_k = st.slider(
                "æ£€ç´¢æ•°é‡", 
                5, 30, 
                value=st.session_state.top_k_value, 
                step=5,
                key="top_k_slider"
            )
            st.session_state.top_k_value = top_k
            
            rerank_top_k = st.slider(
                "é‡æ’åºæ•°é‡", 
                1, 10, 
                value=st.session_state.rerank_top_k_value, 
                step=1,
                key="rerank_top_k_slider"
            )
            st.session_state.rerank_top_k_value = rerank_top_k
            
            min_rerank_score = st.slider("æœ€å°é‡æ’åºåˆ†æ•°", 0.0, 1.0, 0.4, 0.1)

        Config.TOP_K = top_k
        Config.RERANK_TOP_K = rerank_top_k

        # ========= Rank æ¨¡å‹ç®¡ç† =========
        with st.expander("â­ Rankæ¨¡å‹ç®¡ç†", expanded=False):
            if "enable_rank_model" not in st.session_state:
                st.session_state.enable_rank_model = False

            device, device_name = detect_device()
            available_memory, required_memory = get_available_memory_gb(), Config.RERANK_MODEL_MIN_MEMORY_GB
            memory_sufficient = available_memory >= required_memory

            st.info(f"ğŸ“± æ£€æµ‹åˆ°è®¾å¤‡: {device_name}")
            st.info(f"ğŸ’¾ å¯ç”¨å†…å­˜: {available_memory:.2f}GB / éœ€è¦: {required_memory}GB")

            reranker = st.session_state.get("reranker")
            rank_model_file_exists = Path(Config.RERANK_MODEL_PATH).exists()
            rank_model_available = rank_model_file_exists

            if not rank_model_available:
                st.warning("âš ï¸ Rankæ¨¡å‹ä¸å¯ç”¨ï¼ˆæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼‰")
                enable_rank = False
            elif not memory_sufficient:
                st.warning(f"âš ï¸ å†…å­˜ä¸è¶³ï¼éœ€è¦{required_memory}GBï¼Œå½“å‰ä»…{available_memory:.2f}GB")
                enable_rank = False
            else:
                enable_rank = st.checkbox(
                    "å¯ç”¨Ranké‡æ’åºæ¨¡å‹",
                    value=st.session_state.enable_rank_model,
                    help="å¯ç”¨åä¼šä½¿ç”¨AIæ¨¡å‹å¯¹æ£€ç´¢ç»“æœè¿›è¡Œæ™ºèƒ½é‡æ’åºï¼Œå¯èƒ½ä¼šæ¶ˆè€—è¾ƒå¤šå†…å­˜"
                )

            if enable_rank and not st.session_state.enable_rank_model:
                st.session_state.enable_rank_model = True
                if reranker is None:
                    st.info("â„¹ï¸ Rankæ¨¡å‹å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–ï¼Œè¯·å…ˆè¿›è¡Œä¸€æ¬¡å¯¹è¯")
                elif hasattr(reranker, 'load_model'):
                    with st.spinner("æ­£åœ¨åŠ è½½Rankæ¨¡å‹..."):
                        if reranker.load_model():
                            st.success("âœ… Rankæ¨¡å‹åŠ è½½æˆåŠŸ")
                        else:
                            st.error("âŒ Rankæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå·²ç¦ç”¨")
                            st.session_state.enable_rank_model = False
            elif not enable_rank and st.session_state.enable_rank_model:
                st.session_state.enable_rank_model = False
                if reranker is not None and hasattr(reranker, 'unload_model'):
                    reranker.unload_model()

        # ========= æ¨¡å‹çŠ¶æ€ =========
        with st.expander("æ¨¡å‹çŠ¶æ€", expanded=False):
            reranker = st.session_state.get("reranker")
            rank_model_file_exists = Path(Config.RERANK_MODEL_PATH).exists()
            rank_model_available = rank_model_file_exists
            embed_status = "âœ… å·²åŠ è½½" if Path(Config.EMBED_MODEL_PATH).exists() else "âŒ æœªæ‰¾åˆ°"

            if reranker is not None and hasattr(reranker, 'is_loaded') and reranker.is_loaded():
                rerank_status = "âœ… å·²å¯ç”¨"
            elif rank_model_available:
                rerank_status = "â¸ï¸ å·²åˆå§‹åŒ–ï¼ˆæœªå¯ç”¨ï¼‰"
            else:
                rerank_status = "âŒ ä¸å¯ç”¨"

            st.write(f"åµŒå…¥æ¨¡å‹: {embed_status}")
            st.write(f"Rankæ¨¡å‹: {rerank_status}")

        # ========= æ–‡æ¡£è¯´æ˜ =========
        with st.expander("æ–‡æ¡£è¯´æ˜", expanded=False):
            st.write("æŸ¥çœ‹ç³»ç»Ÿä¸»è¦åŠŸèƒ½ã€æ¨¡å‹ä¿¡æ¯åŠ API ç”³è¯·æ–¹å¼ã€‚")
            doc_options = [DOC_PLACEHOLDER] + list(DOC_PAGES.keys())
            current_doc = st.session_state.get("doc_category", DOC_PLACEHOLDER)
            current_index = doc_options.index(current_doc) if current_doc in doc_options else 0
            selected_doc = st.radio(
                "é€‰æ‹©æ–‡æ¡£ç±»åˆ«",
                options=doc_options,
                index=current_index,
            )

            if selected_doc == DOC_PLACEHOLDER:
                if st.session_state.get("show_docs"):
                    st.session_state.show_docs = False
                    st.session_state.doc_category = DOC_PLACEHOLDER
                    st.rerun()
            else:
                should_switch = (
                    selected_doc != st.session_state.get("doc_category")
                    or not st.session_state.get("show_docs", False)
                )
                if should_switch:
                    st.session_state.doc_category = selected_doc
                    st.session_state.show_docs = True
                    st.rerun()

            st.caption("è¯´æ˜æ–‡æ¡£å­˜æ”¾äº `webui/doc_description/`ã€‚")

        # ========= æ³•å¾‹æ£€ç´¢ =========
        if st.button("ğŸ“š æ³•å¾‹æ£€ç´¢", use_container_width=True):
            st.session_state.show_legal_search = True
            st.session_state.show_docs = False
            st.rerun()

        st.info("ğŸ’¡ æç¤ºï¼šDeepSeekæ¨¡å‹éœ€è¦æœ‰æ•ˆçš„API Keyï¼Œå¯åœ¨å®˜ç½‘ç”³è¯·")

        return llm_choice, st.session_state.llm_sub_choice, api_key, temperature, top_p, max_tokens, min_rerank_score



def try_auto_switch_llm(current_choice: str) -> bool:
    """å°è¯•è‡ªåŠ¨åˆ‡æ¢ LLMï¼ˆæŒ‰ä¼˜å…ˆçº§ deepseek -> glm -> qwen -> localï¼‰ï¼Œå¦‚æœåˆ‡æ¢æˆåŠŸè¿”å› Trueã€‚

    åˆ‡æ¢ä¼šè°ƒç”¨ `init_models` å¹¶æ›´æ–° `st.session_state` ä¸ `Settings.llm`ã€‚
    """
    candidates = ["deepseek", "glm", "qwen", "local"]
    remote_key_map = {
        "deepseek": "LLM_API_KEY",
        "glm": "GLM_API_KEY",
        "qwen": "QWEN_API_KEY",
    }
    for cand in candidates:
        if cand == current_choice:
            continue

        # è¿œç«¯æ¨¡å‹éœ€è¦ API Key ä¸”å…¶ api_base å¿…é¡»å¯è¾¾
        if cand in remote_key_map:
            api_key_name = remote_key_map[cand]
            api_key = os.environ.get(api_key_name)
            if not api_key:
                print(f"[try_auto_switch_llm] è·³è¿‡ {cand}ï¼šæœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ {api_key_name}")
                continue

            api_base = LLM_CONFIGS.get(cand, {}).get('api_base')
            if not api_base:
                print(f"[try_auto_switch_llm] è·³è¿‡ {cand}ï¼šæœªé…ç½® api_base")
                continue

            # è½»é‡å¯è¾¾æ€§æ£€æµ‹ï¼ˆå¿«é€Ÿ HTTP è¯·æ±‚ï¼‰
            try:
                resp = requests.get(api_base, timeout=2)
                # ä»…è¦æ±‚èƒ½å¤Ÿå»ºç«‹è¿æ¥å¹¶è¿”å›ï¼Œä¸å¼ºæ±‚ 200
                print(f"[try_auto_switch_llm] {cand} api_base å¯è¾¾: {api_base} (status {resp.status_code})")
            except Exception as e:
                print(f"[try_auto_switch_llm] {cand} api_base ä¸å¯è¾¾: {api_base}ï¼ŒåŸå› : {e}")
                continue

            # å°è¯•åˆå§‹åŒ–æ¨¡å‹
            try:
                embed_model, llm, reranker, chosen = init_models(cand, api_key, None)
                if llm is not None:
                    st.session_state.current_llm_choice = chosen
                    st.session_state.llm = llm
                    st.session_state.embed_model = embed_model
                    st.session_state.reranker = reranker
                    Settings.llm = llm
                    st.success(f"å·²åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹: {chosen}")
                    print(f"[try_auto_switch_llm] å·²åˆ‡æ¢åˆ°å¯ç”¨æ¨¡å‹: {chosen}")
                    return True
            except Exception as e:
                print(f"[try_auto_switch_llm] åˆå§‹åŒ– {cand} å¤±è´¥: {e}")
                traceback.print_exc()
                continue

        # æœ¬åœ°å€™é€‰ï¼šæ£€æŸ¥æœ¬åœ°èŠå¤©æ¨¡å‹ç›®å½•
        if cand == 'local':
            local_models_dir = Path(__file__).parent / 'model' / 'chat_models'
            if not (local_models_dir.exists() and any(local_models_dir.iterdir())):
                print(f"[try_auto_switch_llm] æœ¬åœ°æ¨¡å‹ç›®å½•æ— å¯ç”¨æ¨¡å‹: {local_models_dir}")
                continue
            try:
                embed_model, llm, reranker, chosen = init_models('local', None, None)
                if llm is not None:
                    st.session_state.current_llm_choice = chosen
                    st.session_state.llm = llm
                    st.session_state.embed_model = embed_model
                    st.session_state.reranker = reranker
                    Settings.llm = llm
                    st.success("å·²åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å‹")
                    print("[try_auto_switch_llm] å·²åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å‹")
                    return True
            except Exception as e:
                print(f"[try_auto_switch_llm] åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")
                traceback.print_exc()
                continue

    print("[try_auto_switch_llm] æœªæ‰¾åˆ°å¯ç”¨çš„è¿œç¨‹æ¨¡å‹æˆ–æœ¬åœ°å¤‡é€‰")
    st.warning("æœªæ‰¾åˆ°å¯ç”¨çš„å¤‡ç”¨æ¨¡å‹ã€‚è¯·æ£€æŸ¥ API Key æˆ–æœ¬åœ°æ¨¡å‹ç›®å½•ã€‚")
    return False

# ================== ä¸»ç¨‹åº ==================
def main():
    # ç¦ç”¨ Streamlit æ–‡ä»¶çƒ­é‡è½½ï¼ˆæ”¾åœ¨æ›´å®‰å…¨çš„ä½ç½®ï¼‰
    try:
        disable_streamlit_watcher()
    except Exception as e:
        # å¿½ç•¥è¿™ä¸ªé”™è¯¯ï¼Œä¸å½±å“ä¸»è¦åŠŸèƒ½
        print(f"ç¦ç”¨æ–‡ä»¶ç›‘è§†å™¨æ—¶å‡ºç°è­¦å‘Š: {e}")
    
    # æ£€æŸ¥ç”¨æˆ·è®¤è¯
    if not check_authentication():
        render_login_page()
        st.stop()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºç®¡ç†å‘˜
    if is_admin():
        render_admin_panel()
        st.stop()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºæ¸¸å®¢æ¨¡å¼
    if st.session_state.get("username") == "æ¸¸å®¢":
        render_guest_page()
        st.stop()
    
    st.title("âš–ï¸ æ™ºèƒ½æ³•å¾‹å’¨è¯¢åŠ©æ‰‹")
    st.markdown("æ¬¢è¿ä½¿ç”¨ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹æ™ºèƒ½å’¨è¯¢ç³»ç»Ÿï¼Œè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†åŸºäºæœ€æ–°ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹æ³•è§„ä¸ºæ‚¨è§£ç­”ã€‚")

    # ä¾§è¾¹æ é…ç½®
    llm_choice, llm_sub_choice, api_key, temperature, top_p, max_tokens, min_rerank_score = init_sidebar()
    
    # åœ¨ä¾§è¾¹æ æ˜¾ç¤ºç”¨æˆ·ä¿¡æ¯
    render_user_info_sidebar()
    
    # å¦‚æœå¤„äºæ³•å¾‹æ£€ç´¢æ¨¡å¼ï¼Œåˆ™æ˜¾ç¤ºæ£€ç´¢é¡µé¢
    if st.session_state.get("show_legal_search", False):
        render_legal_search_page()
        st.stop()
    
    # å¦‚æœå¤„äºæ–‡æ¡£æ¨¡å¼ï¼Œåˆ™ç›´æ¥æ˜¾ç¤ºæ–‡æ¡£å¹¶é€€å‡º
    if st.session_state.get("show_docs", False):
        show_documentation_page()
        st.stop()
    
    # æ›´æ–°LLMé…ç½®
    if llm_choice in LLM_CONFIGS:
        LLM_CONFIGS[llm_choice]["temperature"] = temperature
        LLM_CONFIGS[llm_choice]["top_p"] = top_p
        LLM_CONFIGS[llm_choice]["max_tokens"] = max_tokens

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "history" not in st.session_state:
        st.session_state.history = []
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆå½“é…ç½®æ”¹å˜æ—¶ï¼Œæˆ–è€…æ¨¡å‹æœªåˆå§‹åŒ–æ—¶ï¼‰
    current_config = f"{llm_choice}_{llm_sub_choice}_{api_key}_{temperature}_{top_p}_{max_tokens}"
    need_init = (
        "last_config" not in st.session_state
        or st.session_state.last_config != current_config
        or st.session_state.get("reranker") is None
        or st.session_state.get("llm") is None
    )
    
    if need_init:
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹..."):
            embed_model, llm, reranker, current_llm_choice = init_models(llm_choice, api_key, llm_sub_choice)
            st.session_state.last_config = current_config
            st.session_state.current_llm_choice = current_llm_choice
            st.session_state.current_llm_sub_choice = llm_sub_choice
            st.session_state.embed_model = embed_model
            st.session_state.llm = llm
            st.session_state.reranker = reranker
    
    # åˆå§‹åŒ–æ•°æ®
    if not Path(Config.VECTOR_DB_DIR).exists():
        with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“..."):
            raw_data = load_and_validate_json_files(Config.DATA_DIR)
            nodes = create_nodes(raw_data)
    else:
        nodes = None
    
    index = init_vector_store(nodes)
    retriever = index.as_retriever(
        similarity_top_k=Config.TOP_K,
        vector_store_query_mode="hybrid",
        alpha=0.5
    )
    
    response_synthesizer = get_response_synthesizer(verbose=True)
    
    # èŠå¤©ç•Œé¢
    init_chat_interface()
    
    if prompt := st.chat_input("è¯·è¾“å…¥ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹ç›¸å…³é—®é¢˜"):
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²æ­£ç¡®åˆå§‹åŒ–
        if st.session_state.get("llm") is None:
            st.error("âŒ è¯·å…ˆé…ç½®API Keyå¹¶ç¡®ä¿æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            st.stop()
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # å¤„ç†æŸ¥è¯¢ï¼ˆæµå¼è¾“å‡ºï¼‰
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜..."):
            # è°ƒç”¨èŠå¤©æ¨¡å—å¤„ç†æ¶ˆæ¯ï¼ˆæµå¼ï¼Œå†…éƒ¨å·²åˆ›å»º assistant æ¶ˆæ¯å®¹å™¨ï¼‰
            response_text, filtered_nodes, used_rank = handle_chat_message_streaming(
                prompt=prompt,
                retriever=retriever,
                response_synthesizer=response_synthesizer,
                llm_choice=llm_choice,
                min_rerank_score=min_rerank_score,
                try_auto_switch_llm_func=try_auto_switch_llm
            )
            
            # æ˜¾ç¤ºå“åº”ï¼ˆé™„åŠ å†…å®¹å¦‚æ€ç»´é“¾ã€å‚è€ƒä¾æ®ç­‰ï¼‰
            display_chat_response(response_text, filtered_nodes, used_rank)
            
            # è‡ªåŠ¨ä¿å­˜å½“å‰ä¼šè¯
            if "chat_history_manager" in st.session_state and "current_session_id" in st.session_state:
                st.session_state.chat_history_manager.save_session(
                    st.session_state.current_session_id,
                    st.session_state.messages
                )

if __name__ == "__main__":
    main()