# -*- coding: utf-8 -*-
import json
import time
import os
from pathlib import Path
from typing import List, Dict, Optional
import re
import chromadb
import traceback
from dotenv import load_dotenv, set_key

import streamlit as st
import psutil
import torch
import requests
from llama_index.core import VectorStoreIndex, StorageContext, Settings, get_response_synthesizer
from llama_index.core.schema import TextNode, NodeWithScore
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core import QueryBundle
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import PromptTemplate
from llama_index.core.postprocessor import SentenceTransformerRerank
from llama_index.llms.openai_like import OpenAILike

# å¯¼å…¥é…ç½®å’Œæ¨¡å‹é…ç½®
from utils import Config, LLM_CONFIGS, DEEPSEEK_MODELS, GLM_MODELS

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'

# è‡ªåŠ¨åŠ è½½.envæ–‡ä»¶
load_dotenv(dotenv_path=Path(__file__).parent / '.env', override=True)

# ================== Streamlité¡µé¢é…ç½® ==================
st.set_page_config(
    page_title="æ™ºèƒ½æ³•å¾‹å’¨è¯¢åŠ©æ‰‹",
    page_icon="âš–ï¸",
    layout="centered",
    initial_sidebar_state="auto"
)

def disable_streamlit_watcher():
    """æ›´å®‰å…¨çš„æ–¹å¼ç¦ç”¨Streamlitæ–‡ä»¶ç›‘è§†å™¨"""
    try:
        from streamlit import runtime
        if runtime.exists():
            instance = runtime.get_instance()
            def _on_script_changed(_):
                return
            if hasattr(instance, '_on_script_changed'):
                instance._on_script_changed = _on_script_changed
    except Exception as e:
        print(f"ç¦ç”¨æ–‡ä»¶ç›‘è§†å™¨æ—¶å‡ºç°è­¦å‘Š: {e}")

# ================== è®¾å¤‡æ£€æµ‹å’Œå†…å­˜å·¥å…· ==================
def detect_device():
    """æ£€æµ‹è®¾å¤‡æ˜¯å¦æ”¯æŒGPUï¼Œè¿”å›è®¾å¤‡ç±»å‹"""
    if torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0)
        return device, f"GPU ({device_name})"
    else:
        return "cpu", "CPU"

def get_available_memory_gb():
    """è·å–ç³»ç»Ÿå¯ç”¨å†…å­˜ï¼ˆGBï¼‰"""
    memory = psutil.virtual_memory()
    available_gb = memory.available / (1024 ** 3)
    return available_gb

def check_rank_model_memory():
    """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿå†…å­˜åŠ è½½rankæ¨¡å‹"""
    available_memory = get_available_memory_gb()
    required_memory = Config.RERANK_MODEL_MIN_MEMORY_GB
    
    return available_memory >= required_memory, available_memory, required_memory

# ================== ä¿®å¤åçš„è‡ªå®šä¹‰é‡æ’åºå™¨ ==================
class SimpleQwenReranker(BaseNodePostprocessor):
    # ä½¿ç”¨ Pydantic å­—æ®µå®šä¹‰
    model_path: str
    top_n: int = 3
    device: str = "cpu"
    auto_load: bool = False
    
    def __init__(self, model_path: str, top_n: int = 3, device: str = "cpu", auto_load: bool = False):
        # ä½¿ç”¨ Pydantic çš„æ–¹å¼åˆå§‹åŒ–ï¼ˆæ‰€æœ‰å‚æ•°éƒ½å¿…é¡»ä¼ é€’ç»™super().__init__ï¼‰
        super().__init__(model_path=model_path, top_n=top_n, device=device, auto_load=auto_load)
        
        # åˆå§‹åŒ–å†…éƒ¨çŠ¶æ€
        self._is_loaded = False
        self._model = None
        
        # ä»…å½“auto_loadä¸ºTrueæ—¶æ‰åŠ è½½æ¨¡å‹
        if auto_load:
            self._try_load_model()
    
    def _try_load_model(self):
        """å°è¯•åŠ è½½æ¨¡å‹ï¼Œä½†ä¸æŠ›å‡ºå¼‚å¸¸"""
        try:
            from sentence_transformers import CrossEncoder
            self._model = CrossEncoder(
                self.model_path, 
                trust_remote_code=True, 
                local_files_only=True,
                device=self.device
            )
            
            # ä¿®å¤ï¼šè®¾ç½®å¡«å……ä»¤ç‰Œ
            if hasattr(self._model, 'tokenizer') and self._model.tokenizer.pad_token is None:
                self._model.tokenizer.pad_token = self._model.tokenizer.eos_token
            
            self._is_loaded = True
            print(f"âœ… Qwen3-Reranker åŠ è½½æˆåŠŸ (è®¾å¤‡: {self.device}): {self.model_path}")
            
        except Exception as e:
            print(f"âŒ é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self._is_loaded = False
    
    def is_loaded(self):
        return self._is_loaded
    
    def load_model(self):
        """ä¸»åŠ¨åŠ è½½æ¨¡å‹"""
        if not self._is_loaded:
            self._try_load_model()
        return self._is_loaded
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜"""
        if self._model is not None:
            del self._model
            self._model = None
            self._is_loaded = False
            print("âœ… Rankæ¨¡å‹å·²å¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾")
    
    def _postprocess_nodes(self, nodes: List[NodeWithScore], query_bundle: QueryBundle):
        if not nodes or not self._is_loaded or self._model is None:
            return nodes[:self.top_n] if nodes else []
        
        try:
            # å‡†å¤‡æŸ¥è¯¢-æ–‡æ¡£å¯¹
            query_doc_pairs = []
            for node in nodes:
                query_doc_pairs.append([query_bundle.query_str, node.node.get_content()])
            
            # é€ä¸ªå¤„ç†ï¼Œé¿å…æ‰¹é‡å¡«å……é—®é¢˜
            scores = []
            for pair in query_doc_pairs:
                score = self._model.predict([pair])
                scores.append(float(score[0]))
            
            # å°†åˆ†æ•°æ·»åŠ åˆ°èŠ‚ç‚¹
            for node, score in zip(nodes, scores):
                node.score = score
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›å‰top_nä¸ª
            sorted_nodes = sorted(nodes, key=lambda x: x.score, reverse=True)
            return sorted_nodes[:self.top_n]
            
        except Exception as e:
            print(f"é‡æ’åºå¤±è´¥: {e}")
            return nodes[:self.top_n]

# ================== ç¼“å­˜èµ„æºåˆå§‹åŒ– ==================
@st.cache_resource(show_spinner="åˆå§‹åŒ–æ¨¡å‹ä¸­...")
def init_models(llm_choice="deepseek", api_key=None, llm_sub_choice=None):
    # æ£€æŸ¥åµŒå…¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    embed_model_path = Path(Config.EMBED_MODEL_PATH)
    if not embed_model_path.exists():
        st.error(f"âŒ åµŒå…¥æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {Config.EMBED_MODEL_PATH}")
        st.info("è¯·ç¡®ä¿æ¨¡å‹å·²æ­£ç¡®ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„")
        st.stop()
    
    embed_model = HuggingFaceEmbedding(
        model_name=str(embed_model_path),
    )
    
    # æ£€æŸ¥é‡æ’åºæ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆé»˜è®¤ä¸åŠ è½½ï¼‰
    rerank_model_path = Path(Config.RERANK_MODEL_PATH)
    if not rerank_model_path.exists():
        st.warning(f"âš ï¸ é‡æ’åºæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {Config.RERANK_MODEL_PATH}")
        st.info("rankæ¨¡å‹åŠŸèƒ½ä¸å¯ç”¨")
        reranker = None
    else:
        try:
            # æ£€æµ‹è®¾å¤‡
            device, device_name = detect_device()
            
            # åˆ›å»ºé‡æ’åºå™¨å®ä¾‹ï¼Œä½†ä¸è‡ªåŠ¨åŠ è½½ï¼ˆauto_load=Falseï¼‰
            reranker = SimpleQwenReranker(
                model_path=str(rerank_model_path),
                top_n=Config.RERANK_TOP_K,
                device=device,
                auto_load=False  # é»˜è®¤ä¸åŠ è½½
            )
            print(f"âœ… Rankæ¨¡å‹å·²åˆå§‹åŒ–ï¼ˆæœªåŠ è½½ï¼‰, æ£€æµ‹åˆ°è®¾å¤‡: {device_name}")
                
        except Exception as e:
            # æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            import traceback
            error_details = traceback.format_exc()
            st.error(f"âŒ é‡æ’åºæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            st.info("å°†ç¦ç”¨é‡æ’åºåŠŸèƒ½ï¼Œä»…ä½¿ç”¨åŸºç¡€æ£€ç´¢")
            reranker = None
    
    # åŸºç¡€é…ç½®
    config = dict(LLM_CONFIGS[llm_choice])
    
    # æ ¹æ®å­æ¨¡å‹é€‰æ‹©è¦†ç›–é…ç½®ï¼ˆä»… deepseek / glm æ”¯æŒï¼‰
    if llm_choice == "deepseek" and llm_sub_choice and llm_sub_choice in DEEPSEEK_MODELS:
        config.update(DEEPSEEK_MODELS[llm_sub_choice])
    elif llm_choice == "glm" and llm_sub_choice and llm_sub_choice in GLM_MODELS:
        config.update(GLM_MODELS[llm_sub_choice])
    
    if llm_choice == "deepseek":
        if not api_key:
            st.error("âŒ è¯·æä¾›DeepSeek API Key")
            Settings.embed_model = embed_model
            return embed_model, None, reranker, llm_choice
        
        llm = OpenAILike(
            model=config["model"],
            api_base=config["api_base"],
            api_key=api_key,
            context_window=config["context_window"],
            is_chat_model=True,
            is_function_calling_model=False,
            max_tokens=config["max_tokens"],
            temperature=config["temperature"],
            top_p=config["top_p"]
        )
    elif llm_choice == "glm":
        if not api_key:
            st.error("âŒ è¯·æä¾›GLM API Key")
            Settings.embed_model = embed_model
            return embed_model, None, reranker, llm_choice
            
        llm = OpenAILike(
            model=config["model"],
            api_base=config["api_base"],
            api_key=api_key,
            context_window=config["context_window"],
            is_chat_model=True,
            is_function_calling_model=False,
            max_tokens=config["max_tokens"],
            temperature=config["temperature"],
            top_p=config["top_p"]
        )
    else:  # local
        llm = OpenAILike(
            model=config["model"],
            api_base=config["api_base"],
            api_key="fake",
            context_window=config["context_window"],
            is_chat_model=True,
            is_function_calling_model=False,
            max_tokens=config["max_tokens"],
            temperature=config["temperature"],
            top_p=config["top_p"]
        )
    
    Settings.embed_model = embed_model
    Settings.llm = llm
    
    return embed_model, llm, reranker, llm_choice

@st.cache_resource(show_spinner="åŠ è½½çŸ¥è¯†åº“ä¸­...")
def init_vector_store(_nodes):
    chroma_client = chromadb.PersistentClient(path=Config.VECTOR_DB_DIR)
    chroma_collection = chroma_client.get_or_create_collection(
        name=Config.COLLECTION_NAME,
        metadata={"hnsw:space": "cosine"}
    )

    if chroma_collection.count() == 0 and _nodes is not None:
        # æ–°å»ºç´¢å¼•
        storage_context = StorageContext.from_defaults(
            vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
        )
        storage_context.docstore.add_documents(_nodes)  
        index = VectorStoreIndex(
            _nodes,
            storage_context=storage_context,
            show_progress=True
        )
        # åˆ›å»ºpersistç›®å½•
        Path(Config.PERSIST_DIR).mkdir(parents=True, exist_ok=True)
        storage_context.persist(persist_dir=Config.PERSIST_DIR)
        index.storage_context.persist(persist_dir=Config.PERSIST_DIR)
    else:
        # åŠ è½½ç°æœ‰ç´¢å¼•
        storage_context = StorageContext.from_defaults(
            vector_store=ChromaVectorStore(chroma_collection=chroma_collection)
        )
        index = VectorStoreIndex.from_vector_store(
            storage_context.vector_store,
            storage_context=storage_context,
            embed_model=Settings.embed_model
        )
    return index

# ================== æ•°æ®å¤„ç† ==================
def load_and_validate_json_files(data_dir: str) -> List[Dict]:
    """åŠ è½½å¹¶éªŒè¯JSONæ³•å¾‹æ–‡ä»¶"""
    json_files = list(Path(data_dir).glob("*.json"))
    assert json_files, f"æœªæ‰¾åˆ°JSONæ–‡ä»¶äº {data_dir}"
    
    all_data = []
    for json_file in json_files:
        with open(json_file, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                # éªŒè¯æ•°æ®ç»“æ„
                if not isinstance(data, list):
                    raise ValueError(f"æ–‡ä»¶ {json_file.name} æ ¹å…ƒç´ åº”ä¸ºåˆ—è¡¨")
                for item in data:
                    if not isinstance(item, dict):
                        raise ValueError(f"æ–‡ä»¶ {json_file.name} åŒ…å«éå­—å…¸å…ƒç´ ")
                    for k, v in item.items():
                        if not isinstance(v, str):
                            raise ValueError(f"æ–‡ä»¶ {json_file.name} ä¸­é”® '{k}' çš„å€¼ä¸æ˜¯å­—ç¬¦ä¸²")
                all_data.extend({
                    "content": item,
                    "metadata": {"source": json_file.name}
                } for item in data)
            except Exception as e:
                raise RuntimeError(f"åŠ è½½æ–‡ä»¶ {json_file} å¤±è´¥: {str(e)}")
    
    print(f"æˆåŠŸåŠ è½½ {len(all_data)} ä¸ªæ³•å¾‹æ–‡ä»¶æ¡ç›®")
    return all_data

def create_nodes(raw_data: List[Dict]) -> List[TextNode]:
    """æ·»åŠ IDç¨³å®šæ€§ä¿éšœ"""
    nodes = []
    for entry in raw_data:
        law_dict = entry["content"]
        source_file = entry["metadata"]["source"]
        
        for full_title, content in law_dict.items():
            # ç”Ÿæˆç¨³å®šIDï¼ˆé¿å…é‡å¤ï¼‰
            node_id = f"{source_file}::{full_title}"
            
            parts = full_title.split(" ", 1)
            law_name = parts[0] if len(parts) > 0 else "æœªçŸ¥æ³•å¾‹"
            article = parts[1] if len(parts) > 1 else "æœªçŸ¥æ¡æ¬¾"
            
            node = TextNode(
                text=content,
                id_=node_id,  # æ˜¾å¼è®¾ç½®ç¨³å®šID
                metadata={
                    "law_name": law_name,
                    "article": article,
                    "full_title": full_title,
                    "source_file": source_file,
                    "content_type": "legal_article"
                }
            )
            nodes.append(node)
    
    print(f"ç”Ÿæˆ {len(nodes)} ä¸ªæ–‡æœ¬èŠ‚ç‚¹ï¼ˆIDç¤ºä¾‹ï¼š{nodes[0].id_}ï¼‰")
    return nodes

# ================== ç•Œé¢ç»„ä»¶ ==================
def init_sidebar():
    """ä¾§è¾¹æ é…ç½®"""
    with st.sidebar:
        st.header("âš™ï¸ åŠŸèƒ½æ¨¡å—")
        
        # é»˜è®¤å€¼åˆå§‹åŒ–ï¼Œé¿å…æœªè¿›å…¥æŠ˜å é¢æ¿æ—¶è¿”å› None
        temperature = 0.3
        top_p = 0.7
        max_tokens = 1024
        top_k = Config.TOP_K
        rerank_top_k = Config.RERANK_TOP_K
        min_rerank_score = 0.4
        api_key = None
        
        # ========= æ¨¡å‹é…ç½® =========
        with st.expander("æ¨¡å‹é…ç½®", expanded=False):
            # LLMé€‰æ‹©ï¼ˆä¿å­˜åˆ° session_stateï¼Œä¾¿äºæŒ‰é’®åˆ‡æ¢ï¼‰
            if 'llm_choice_requested' in st.session_state:
                requested = st.session_state.pop('llm_choice_requested')
                st.session_state.llm_choice_select = requested

            if 'llm_choice_select' not in st.session_state:
                st.session_state.llm_choice_select = 'deepseek'

            llm_choice = st.selectbox(
                "é€‰æ‹©LLMæ¨¡å‹",
                options=["deepseek", "glm", "local"],
                format_func=lambda x: {
                    "deepseek": "DeepSeek",
                    "glm": "æ™ºè°±GLM", 
                    "local": "æœ¬åœ°æ¨¡å‹"
                }[x],
                key='llm_choice_select'
            )

            env_path = str(Path(__file__).parent / '.env')
            if llm_choice == 'deepseek':
                current = os.environ.get('LLM_API_KEY', '')
                api_input = st.text_input('DeepSeek API Key', value=current, type='password', help='DeepSeek API Keyï¼Œç•™ç©ºåˆ™ä¸èƒ½è°ƒç”¨ DeepSeek')
                if api_input and api_input != current:
                    try:
                        set_key(env_path, 'LLM_API_KEY', api_input)
                        os.environ['LLM_API_KEY'] = api_input
                    except Exception as e:
                        print(f"å†™ .env å¤±è´¥: {e}")
                api_key = os.environ.get('LLM_API_KEY')
            elif llm_choice == 'glm':
                current = os.environ.get('GLM_API_KEY', '')
                api_input = st.text_input('GLM API Key', value=current, type='password', help='GLM API Keyï¼Œç•™ç©ºåˆ™ä¸èƒ½è°ƒç”¨ GLM')
                if api_input and api_input != current:
                    try:
                        set_key(env_path, 'GLM_API_KEY', api_input)
                        os.environ['GLM_API_KEY'] = api_input
                    except Exception as e:
                        print(f"å†™ .env å¤±è´¥: {e}")
                api_key = os.environ.get('GLM_API_KEY')
            else:
                local_models_dir = Path(__file__).parent / 'model' / 'chat_models'
                local_available = local_models_dir.exists() and any(local_models_dir.iterdir())
                if not local_available:
                    st.warning(f"âš ï¸ æœªæ£€æµ‹åˆ°æœ¬åœ°èŠå¤©æ¨¡å‹äº: {local_models_dir}ã€‚è¯·å…ˆå°†æ¨¡å‹æ”¾å…¥è¯¥ç›®å½•ï¼Œæˆ–åˆ‡æ¢åˆ°äº‘ç«¯æ¨¡å‹ã€‚")
                    col1, col2 = st.columns(2)
                    if col1.button('åˆ‡æ¢åˆ° DeepSeek'):
                        st.session_state.llm_choice_requested = 'deepseek'
                    if col2.button('åˆ‡æ¢åˆ° GLM'):
                        st.session_state.llm_choice_requested = 'glm'

            # å­æ¨¡å‹é€‰æ‹©
            llm_sub_choice = None
            if llm_choice == "deepseek":
                if "llm_sub_choice" not in st.session_state:
                    st.session_state.llm_sub_choice = "deepseek-chat"
                options = list(DEEPSEEK_MODELS.keys())
                llm_sub_choice = st.selectbox(
                    "DeepSeek å­æ¨¡å‹",
                    options=options,
                    index=options.index(st.session_state.llm_sub_choice) if st.session_state.llm_sub_choice in options else 0,
                )
                st.session_state.llm_sub_choice = llm_sub_choice
            elif llm_choice == "glm":
                if "llm_sub_choice" not in st.session_state:
                    st.session_state.llm_sub_choice = "glm-4"
                options = list(GLM_MODELS.keys())
                llm_sub_choice = st.selectbox(
                    "GLM å­æ¨¡å‹",
                    options=options,
                    index=options.index(st.session_state.llm_sub_choice) if st.session_state.llm_sub_choice in options else 0,
                )
                st.session_state.llm_sub_choice = llm_sub_choice
            else:
                st.session_state.llm_sub_choice = None

        # ========= æ¨¡å‹å‚æ•° =========
        with st.expander("æ¨¡å‹å‚æ•°", expanded=False):
            temperature = st.slider("Temperature", 0.0, 1.0, 0.3, 0.1)
            top_p = st.slider("Top P", 0.0, 1.0, 0.7, 0.1)
            max_tokens = st.slider("æœ€å¤§ç”Ÿæˆé•¿åº¦", 512, 4096, 1024, 128)

        # ========= æ£€ç´¢å‚æ•° =========
        with st.expander("æ£€ç´¢å‚æ•°", expanded=False):
            top_k = st.slider("æ£€ç´¢æ•°é‡", 5, 30, Config.TOP_K, 5)
            rerank_top_k = st.slider("é‡æ’åºæ•°é‡", 1, 10, Config.RERANK_TOP_K, 1)
            min_rerank_score = st.slider("æœ€å°é‡æ’åºåˆ†æ•°", 0.0, 1.0, 0.4, 0.1)

        Config.TOP_K = top_k
        Config.RERANK_TOP_K = rerank_top_k

        # ========= Rank æ¨¡å‹ç®¡ç† =========
        with st.expander("â­ Rankæ¨¡å‹ç®¡ç†", expanded=False):
            if "enable_rank_model" not in st.session_state:
                st.session_state.enable_rank_model = False

            device, device_name = detect_device()
            available_memory, required_memory = get_available_memory_gb(), Config.RERANK_MODEL_MIN_MEMORY_GB
            memory_sufficient = available_memory >= required_memory

            st.info(f"ğŸ“± æ£€æµ‹åˆ°è®¾å¤‡: {device_name}")
            st.info(f"ğŸ’¾ å¯ç”¨å†…å­˜: {available_memory:.2f}GB / éœ€è¦: {required_memory}GB")

            reranker = st.session_state.get("reranker")
            rank_model_file_exists = Path(Config.RERANK_MODEL_PATH).exists()
            rank_model_available = rank_model_file_exists

            if not rank_model_available:
                st.warning("âš ï¸ Rankæ¨¡å‹ä¸å¯ç”¨ï¼ˆæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼‰")
                enable_rank = False
            elif not memory_sufficient:
                st.warning(f"âš ï¸ å†…å­˜ä¸è¶³ï¼éœ€è¦{required_memory}GBï¼Œå½“å‰ä»…{available_memory:.2f}GB")
                enable_rank = False
            else:
                enable_rank = st.checkbox(
                    "å¯ç”¨Ranké‡æ’åºæ¨¡å‹",
                    value=st.session_state.enable_rank_model,
                    help="å¯ç”¨åä¼šä½¿ç”¨AIæ¨¡å‹å¯¹æ£€ç´¢ç»“æœè¿›è¡Œæ™ºèƒ½é‡æ’åºï¼Œå¯èƒ½ä¼šæ¶ˆè€—è¾ƒå¤šå†…å­˜"
                )

            if enable_rank and not st.session_state.enable_rank_model:
                st.session_state.enable_rank_model = True
                if reranker is None:
                    st.info("â„¹ï¸ Rankæ¨¡å‹å°†åœ¨é¦–æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–ï¼Œè¯·å…ˆè¿›è¡Œä¸€æ¬¡å¯¹è¯")
                elif hasattr(reranker, 'load_model'):
                    with st.spinner("æ­£åœ¨åŠ è½½Rankæ¨¡å‹..."):
                        if reranker.load_model():
                            st.success("âœ… Rankæ¨¡å‹åŠ è½½æˆåŠŸ")
                        else:
                            st.error("âŒ Rankæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå·²ç¦ç”¨")
                            st.session_state.enable_rank_model = False
            elif not enable_rank and st.session_state.enable_rank_model:
                st.session_state.enable_rank_model = False
                if reranker is not None and hasattr(reranker, 'unload_model'):
                    reranker.unload_model()

        # ========= æ¨¡å‹çŠ¶æ€ =========
        with st.expander("æ¨¡å‹çŠ¶æ€", expanded=False):
            reranker = st.session_state.get("reranker")
            rank_model_file_exists = Path(Config.RERANK_MODEL_PATH).exists()
            rank_model_available = rank_model_file_exists
            embed_status = "âœ… å·²åŠ è½½" if Path(Config.EMBED_MODEL_PATH).exists() else "âŒ æœªæ‰¾åˆ°"

            if reranker is not None and hasattr(reranker, 'is_loaded') and reranker.is_loaded():
                rerank_status = "âœ… å·²å¯ç”¨"
            elif rank_model_available:
                rerank_status = "â¸ï¸ å·²åˆå§‹åŒ–ï¼ˆæœªå¯ç”¨ï¼‰"
            else:
                rerank_status = "âŒ ä¸å¯ç”¨"

            st.write(f"åµŒå…¥æ¨¡å‹: {embed_status}")
            st.write(f"Rankæ¨¡å‹: {rerank_status}")

        st.info("ğŸ’¡ æç¤ºï¼šDeepSeekæ¨¡å‹éœ€è¦æœ‰æ•ˆçš„API Keyï¼Œå¯åœ¨å®˜ç½‘ç”³è¯·")

        return llm_choice, st.session_state.llm_sub_choice, api_key, temperature, top_p, max_tokens, min_rerank_score

def init_chat_interface():
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    for msg in st.session_state.messages:
        role = msg["role"]
        content = msg.get("cleaned", msg["content"])  # ä¼˜å…ˆä½¿ç”¨æ¸…ç†åçš„å†…å®¹
        
        with st.chat_message(role):
            st.markdown(content)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å«æ€ç»´é“¾
            if role == "assistant" and msg.get("think"):
                with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆå†å²å¯¹è¯ï¼‰"):
                    for think_content in msg["think"]:
                        st.markdown(f'<span style="color: #808080">{think_content.strip()}</span>',
                                  unsafe_allow_html=True)
            
            # å¦‚æœæ˜¯åŠ©æ‰‹æ¶ˆæ¯ä¸”æœ‰å‚è€ƒä¾æ®ï¼ˆéœ€è¦ä¿æŒåŸæœ‰å‚è€ƒä¾æ®é€»è¾‘ï¼‰
            if role == "assistant" and "reference_nodes" in msg:
                show_reference_details(msg["reference_nodes"])

def show_reference_details(nodes):
    with st.expander("æŸ¥çœ‹æ”¯æŒä¾æ®"):
        for idx, node in enumerate(nodes, 1):
            meta = node.node.metadata
            st.markdown(f"**[{idx}] {meta['full_title']}**")
            st.caption(f"æ¥æºæ–‡ä»¶ï¼š{meta['source_file']} | æ³•å¾‹åç§°ï¼š{meta['law_name']}")
            st.markdown(f"ç›¸å…³åº¦ï¼š`{node.score:.4f}`")
            st.info(f"{node.node.text}")


def synthesize_with_retries(synthesizer, prompt: str, nodes: List, retries: int = 3, initial_delay: float = 2.0):
    """å¯¹ response_synthesizer.synthesize æ·»åŠ æœ‰é™é‡è¯•å’ŒæŒ‡æ•°é€€é¿ã€‚

    å‚æ•°:
        synthesizer: response_synthesizer å®ä¾‹
        prompt: ç”¨æˆ·è¾“å…¥
        nodes: ç”¨äºåˆæˆçš„èŠ‚ç‚¹åˆ—è¡¨
        retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        initial_delay: åˆå§‹ç­‰å¾…ç§’æ•°ï¼Œåç»­æŒ‰ 2^n æŒ‡æ•°å¢é•¿
    è¿”å›:
        åˆæˆå™¨è¿”å›çš„å¯¹è±¡ï¼ˆä¸åŸ synthesize è¿”å›ç›¸åŒï¼‰
    æŠ›å‡º:
        æœ€åä¸€æ¬¡å¼‚å¸¸ï¼ˆè‹¥å…¨éƒ¨é‡è¯•å¤±è´¥ï¼‰
    """
    last_exc = None
    for attempt in range(1, retries + 1):
        try:
            return synthesizer.synthesize(prompt, nodes=nodes)
        except Exception as e:
            last_exc = e
            print(f"[synthesize_with_retries] å°è¯• {attempt}/{retries} å¤±è´¥: {e}")
            traceback.print_exc()
            if attempt == retries:
                # é‡è¯•å®Œæ¯•ï¼Œé‡å¤æŠ›å‡ºæœ€åçš„å¼‚å¸¸
                raise
            # æŒ‡æ•°é€€é¿
            wait = initial_delay * (2 ** (attempt - 1))
            print(f"[synthesize_with_retries] ç­‰å¾… {wait}s åé‡è¯•...")
            time.sleep(wait)


def is_legal_related(question: str, llm) -> bool:
    """åˆ¤æ–­ç”¨æˆ·é—®é¢˜æ˜¯å¦ä¸æ³•å¾‹ç›¸å…³
    
    å‚æ•°:
        question: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        llm: LLMå®ä¾‹ï¼Œç”¨äºåˆ¤æ–­
    è¿”å›:
        True: ä¸æ³•å¾‹ç›¸å…³ï¼Œéœ€è¦å¯ç”¨æ£€ç´¢
        False: ä¸æ³•å¾‹æ— å…³ï¼Œç›´æ¥ä½¿ç”¨å¯¹è¯æ¨¡å‹å›ç­”
    """
    try:
        # æ„å»ºåˆ¤æ–­æç¤º
        judgment_prompt = f"""è¯·åˆ¤æ–­ä»¥ä¸‹é—®é¢˜æ˜¯å¦ä¸æ³•å¾‹ã€æ³•è§„ã€æ³•å¾‹å’¨è¯¢ã€æ³•å¾‹é—®é¢˜ç›¸å…³ã€‚

é—®é¢˜ï¼š{question}

è¯·åªå›ç­”"æ˜¯"æˆ–"å¦"ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚

å¦‚æœé—®é¢˜æ¶‰åŠï¼š
- æ³•å¾‹æ³•è§„ã€æ³•å¾‹æ¡æ–‡ã€æ³•å¾‹æ¡æ¬¾
- æ³•å¾‹å’¨è¯¢ã€æ³•å¾‹é—®é¢˜ã€æ³•å¾‹çº çº·
- åˆåŒã€åè®®ã€æ³•å¾‹æ–‡ä»¶
- è¯‰è®¼ã€ä»²è£ã€æ³•å¾‹ç¨‹åº
- æ³•å¾‹æƒåˆ©ã€æ³•å¾‹ä¹‰åŠ¡ã€æ³•å¾‹è´£ä»»
- ä»»ä½•éœ€è¦å‚è€ƒæ³•å¾‹æ¡æ–‡æ¥å›ç­”çš„é—®é¢˜

è¯·å›ç­”"æ˜¯"ã€‚

å¦‚æœé—®é¢˜æ˜¯ä¸€èˆ¬æ€§å¯¹è¯ã€é—²èŠã€éæ³•å¾‹ç›¸å…³çš„æŠ€æœ¯é—®é¢˜ã€ç”Ÿæ´»å¸¸è¯†ç­‰ï¼Œè¯·å›ç­”"å¦"ã€‚

å›ç­”ï¼š"""
        
        # è°ƒç”¨LLMè¿›è¡Œåˆ¤æ–­
        response = llm.complete(judgment_prompt)
        result = response.text.strip().lower()
        
        # è§£æç»“æœ
        if "æ˜¯" in result or "yes" in result or "true" in result or "1" in result:
            print(f"[is_legal_related] åˆ¤æ–­ç»“æœï¼šä¸æ³•å¾‹ç›¸å…³")
            return True
        else:
            print(f"[is_legal_related] åˆ¤æ–­ç»“æœï¼šä¸æ³•å¾‹æ— å…³")
            return False
            
    except Exception as e:
        # å¦‚æœåˆ¤æ–­å¤±è´¥ï¼Œé»˜è®¤å¯ç”¨æ£€ç´¢ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        print(f"[is_legal_related] åˆ¤æ–­å¤±è´¥: {e}ï¼Œé»˜è®¤å¯ç”¨æ£€ç´¢")
        traceback.print_exc()
        return True


def try_auto_switch_llm(current_choice: str) -> bool:
    """å°è¯•è‡ªåŠ¨åˆ‡æ¢ LLMï¼ˆæŒ‰ä¼˜å…ˆçº§ deepseek -> glm -> localï¼‰ï¼Œå¦‚æœåˆ‡æ¢æˆåŠŸè¿”å› Trueã€‚

    åˆ‡æ¢ä¼šè°ƒç”¨ `init_models` å¹¶æ›´æ–° `st.session_state` ä¸ `Settings.llm`ã€‚
    """
    candidates = ["deepseek", "glm", "local"]
    for cand in candidates:
        if cand == current_choice:
            continue

        # è¿œç«¯æ¨¡å‹éœ€è¦ API Key ä¸”å…¶ api_base å¿…é¡»å¯è¾¾
        if cand in ("deepseek", "glm"):
            api_key_name = 'LLM_API_KEY' if cand == 'deepseek' else 'GLM_API_KEY'
            api_key = os.environ.get(api_key_name)
            if not api_key:
                print(f"[try_auto_switch_llm] è·³è¿‡ {cand}ï¼šæœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ {api_key_name}")
                continue

            api_base = LLM_CONFIGS.get(cand, {}).get('api_base')
            if not api_base:
                print(f"[try_auto_switch_llm] è·³è¿‡ {cand}ï¼šæœªé…ç½® api_base")
                continue

            # è½»é‡å¯è¾¾æ€§æ£€æµ‹ï¼ˆå¿«é€Ÿ HTTP è¯·æ±‚ï¼‰
            try:
                resp = requests.get(api_base, timeout=2)
                # ä»…è¦æ±‚èƒ½å¤Ÿå»ºç«‹è¿æ¥å¹¶è¿”å›ï¼Œä¸å¼ºæ±‚ 200
                print(f"[try_auto_switch_llm] {cand} api_base å¯è¾¾: {api_base} (status {resp.status_code})")
            except Exception as e:
                print(f"[try_auto_switch_llm] {cand} api_base ä¸å¯è¾¾: {api_base}ï¼ŒåŸå› : {e}")
                continue

            # å°è¯•åˆå§‹åŒ–æ¨¡å‹
            try:
                embed_model, llm, reranker, chosen = init_models(cand, api_key, None)
                if llm is not None:
                    st.session_state.current_llm_choice = chosen
                    st.session_state.llm = llm
                    st.session_state.embed_model = embed_model
                    st.session_state.reranker = reranker
                    Settings.llm = llm
                    st.success(f"å·²åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹: {chosen}")
                    print(f"[try_auto_switch_llm] å·²åˆ‡æ¢åˆ°å¯ç”¨æ¨¡å‹: {chosen}")
                    return True
            except Exception as e:
                print(f"[try_auto_switch_llm] åˆå§‹åŒ– {cand} å¤±è´¥: {e}")
                traceback.print_exc()
                continue

        # æœ¬åœ°å€™é€‰ï¼šæ£€æŸ¥æœ¬åœ°èŠå¤©æ¨¡å‹ç›®å½•
        if cand == 'local':
            local_models_dir = Path(__file__).parent / 'model' / 'chat_models'
            if not (local_models_dir.exists() and any(local_models_dir.iterdir())):
                print(f"[try_auto_switch_llm] æœ¬åœ°æ¨¡å‹ç›®å½•æ— å¯ç”¨æ¨¡å‹: {local_models_dir}")
                continue
            try:
                embed_model, llm, reranker, chosen = init_models('local', None, None)
                if llm is not None:
                    st.session_state.current_llm_choice = chosen
                    st.session_state.llm = llm
                    st.session_state.embed_model = embed_model
                    st.session_state.reranker = reranker
                    Settings.llm = llm
                    st.success("å·²åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å‹")
                    print("[try_auto_switch_llm] å·²åˆ‡æ¢åˆ°æœ¬åœ°æ¨¡å‹")
                    return True
            except Exception as e:
                print(f"[try_auto_switch_llm] åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")
                traceback.print_exc()
                continue

    print("[try_auto_switch_llm] æœªæ‰¾åˆ°å¯ç”¨çš„è¿œç¨‹æ¨¡å‹æˆ–æœ¬åœ°å¤‡é€‰")
    st.warning("æœªæ‰¾åˆ°å¯ç”¨çš„å¤‡ç”¨æ¨¡å‹ã€‚è¯·æ£€æŸ¥ API Key æˆ–æœ¬åœ°æ¨¡å‹ç›®å½•ã€‚")
    return False

# ================== ä¸»ç¨‹åº ==================
def main():
    # ç¦ç”¨ Streamlit æ–‡ä»¶çƒ­é‡è½½ï¼ˆæ”¾åœ¨æ›´å®‰å…¨çš„ä½ç½®ï¼‰
    try:
        disable_streamlit_watcher()
    except Exception as e:
        # å¿½ç•¥è¿™ä¸ªé”™è¯¯ï¼Œä¸å½±å“ä¸»è¦åŠŸèƒ½
        print(f"ç¦ç”¨æ–‡ä»¶ç›‘è§†å™¨æ—¶å‡ºç°è­¦å‘Š: {e}")
    
    st.title("âš–ï¸ æ™ºèƒ½æ³•å¾‹å’¨è¯¢åŠ©æ‰‹")
    st.markdown("æ¬¢è¿ä½¿ç”¨ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹æ™ºèƒ½å’¨è¯¢ç³»ç»Ÿï¼Œè¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä»¬å°†åŸºäºæœ€æ–°ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹æ³•è§„ä¸ºæ‚¨è§£ç­”ã€‚")

    # ä¾§è¾¹æ é…ç½®
    llm_choice, llm_sub_choice, api_key, temperature, top_p, max_tokens, min_rerank_score = init_sidebar()
    
    # æ›´æ–°LLMé…ç½®
    if llm_choice in LLM_CONFIGS:
        LLM_CONFIGS[llm_choice]["temperature"] = temperature
        LLM_CONFIGS[llm_choice]["top_p"] = top_p
        LLM_CONFIGS[llm_choice]["max_tokens"] = max_tokens

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if "history" not in st.session_state:
        st.session_state.history = []
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼ˆå½“é…ç½®æ”¹å˜æ—¶ï¼Œæˆ–è€…æ¨¡å‹æœªåˆå§‹åŒ–æ—¶ï¼‰
    current_config = f"{llm_choice}_{llm_sub_choice}_{api_key}_{temperature}_{top_p}_{max_tokens}"
    need_init = (
        "last_config" not in st.session_state
        or st.session_state.last_config != current_config
        or st.session_state.get("reranker") is None
        or st.session_state.get("llm") is None
    )
    
    if need_init:
        with st.spinner("æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹..."):
            embed_model, llm, reranker, current_llm_choice = init_models(llm_choice, api_key, llm_sub_choice)
            st.session_state.last_config = current_config
            st.session_state.current_llm_choice = current_llm_choice
            st.session_state.current_llm_sub_choice = llm_sub_choice
            st.session_state.embed_model = embed_model
            st.session_state.llm = llm
            st.session_state.reranker = reranker
    
    # åˆå§‹åŒ–æ•°æ®
    if not Path(Config.VECTOR_DB_DIR).exists():
        with st.spinner("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“..."):
            raw_data = load_and_validate_json_files(Config.DATA_DIR)
            nodes = create_nodes(raw_data)
    else:
        nodes = None
    
    index = init_vector_store(nodes)
    retriever = index.as_retriever(
        similarity_top_k=Config.TOP_K,
        vector_store_query_mode="hybrid",
        alpha=0.5
    )
    
    response_synthesizer = get_response_synthesizer(verbose=True)
    
    # èŠå¤©ç•Œé¢
    init_chat_interface()
    
    if prompt := st.chat_input("è¯·è¾“å…¥ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹ç›¸å…³é—®é¢˜"):
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²æ­£ç¡®åˆå§‹åŒ–
        if st.session_state.get("llm") is None:
            st.error("âŒ è¯·å…ˆé…ç½®API Keyå¹¶ç¡®ä¿æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            st.stop()
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # å¤„ç†æŸ¥è¯¢
        with st.spinner("æ­£åœ¨åˆ†æé—®é¢˜..."):
            start_time = time.time()
            
            # é¦–å…ˆåˆ¤æ–­é—®é¢˜æ˜¯å¦ä¸æ³•å¾‹ç›¸å…³
            llm = st.session_state.get("llm")
            if llm is None:
                st.error("âŒ LLMæœªåˆå§‹åŒ–")
                st.stop()
            
            is_legal = is_legal_related(prompt, llm)
            
            if not is_legal:
                # ä¸æ³•å¾‹æ— å…³ï¼Œç›´æ¥ä½¿ç”¨å¯¹è¯æ¨¡å‹å›ç­”
                st.info("ğŸ’¬ æ£€æµ‹åˆ°é—®é¢˜ä¸æ³•å¾‹æ— å…³ï¼Œä½¿ç”¨å¯¹è¯æ¨¡å¼å›ç­”")
                try:
                    response = llm.complete(prompt)
                    response_text = response.text
                    filtered_nodes = []  # éæ³•å¾‹é—®é¢˜æ²¡æœ‰å‚è€ƒä¾æ®
                except Exception as e:
                    print(f"[main] ç›´æ¥å¯¹è¯æ¨¡å¼å¤±è´¥: {e}")
                    traceback.print_exc()
                    response_text = f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}"
                    filtered_nodes = []
            else:
                # ä¸æ³•å¾‹ç›¸å…³ï¼Œå¯ç”¨æ£€ç´¢æµç¨‹
                st.info("âš–ï¸ æ£€æµ‹åˆ°é—®é¢˜ä¸æ³•å¾‹ç›¸å…³ï¼Œå¯ç”¨æ³•å¾‹æ£€ç´¢æ¨¡å¼")
                
                # æ£€ç´¢æµç¨‹
                initial_nodes = retriever.retrieve(prompt)
                
                # ä½¿ç”¨ä¼šè¯çŠ¶æ€ä¸­çš„ rerankerï¼ˆä»…åœ¨å¯ç”¨ä¸”å·²åŠ è½½æ—¶ä½¿ç”¨ï¼‰
                reranker = st.session_state.reranker
                enable_rank = st.session_state.get("enable_rank_model", False)
                
                if enable_rank and reranker is not None and hasattr(reranker, 'is_loaded') and reranker.is_loaded():
                    try:
                        reranked_nodes = reranker.postprocess_nodes(initial_nodes, query_str=prompt)
                        # è¿‡æ»¤èŠ‚ç‚¹
                        filtered_nodes = [node for node in reranked_nodes if node.score > min_rerank_score]
                        st.success("âœ… å·²ä½¿ç”¨é‡æ’åºåŠŸèƒ½")
                    except Exception as e:
                        st.warning(f"âš ï¸ é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€æ£€ç´¢ç»“æœ")
                        filtered_nodes = initial_nodes[:Config.RERANK_TOP_K]
                else:
                    # å¦‚æœæ²¡æœ‰å¯ç”¨é‡æ’åºæ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨åˆå§‹èŠ‚ç‚¹
                    st.info("âš ï¸ Rankæ¨¡å‹æœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€æ£€ç´¢ç»“æœ")
                    filtered_nodes = initial_nodes[:Config.RERANK_TOP_K]  # å–å‰å‡ ä¸ªèŠ‚ç‚¹
                
                if not filtered_nodes:
                    response_text = "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ³•å¾‹æ¡æ–‡ï¼Œè¯·å°è¯•è°ƒæ•´é—®é¢˜æè¿°æˆ–å’¨è¯¢ä¸“ä¸šå¾‹å¸ˆã€‚"
                else:
                    # ç”Ÿæˆå›ç­”ï¼ˆå®‰å…¨è°ƒç”¨ï¼šå¸¦é‡è¯•ä¸å›é€€ï¼‰
                    try:
                        response = synthesize_with_retries(response_synthesizer, prompt, filtered_nodes, retries=3)
                        response_text = response.response
                    except Exception as e:
                        # æ‰“å°è¯¦ç»†è·Ÿè¸ªä»¥ä¾¿è°ƒè¯•
                        print("[main] response_synthesizer ç”Ÿæˆå¤±è´¥ï¼Œè¿›å…¥å›é€€é€»è¾‘:")
                        traceback.print_exc()
                        # å‘ç”¨æˆ·æ˜¾ç¤ºå‹å¥½æç¤º
                        st.error("âš ï¸ åç«¯æ¨¡å‹æœåŠ¡å¼‚å¸¸ï¼Œæ­£åœ¨å°è¯•åˆ‡æ¢å¤‡ç”¨æ¨¡å‹æˆ–å›é€€ä¸ºä¸´æ—¶ç»“æœã€‚")

                        # ä¼˜å…ˆå°è¯•è‡ªåŠ¨åˆ‡æ¢åˆ°å…¶å®ƒå¯ç”¨æ¨¡å‹å¹¶é‡è¯•ä¸€æ¬¡
                        switched = False
                        try:
                            switched = try_auto_switch_llm(st.session_state.get('current_llm_choice', llm_choice))
                        except Exception as e_switch:
                            print(f"[main] è‡ªåŠ¨åˆ‡æ¢æ¨¡å‹è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e_switch}")

                        if switched:
                            try:
                                # ä½¿ç”¨æ–°çš„ LLM é‡æ–°åˆ›å»ºåˆæˆå™¨å¹¶é‡è¯•
                                response_synthesizer = get_response_synthesizer(verbose=True)
                                response = synthesize_with_retries(response_synthesizer, prompt, filtered_nodes, retries=2)
                                response_text = response.response
                                # å¦‚æœæˆåŠŸåˆ™è·³è¿‡åç»­å›é€€é€»è¾‘
                            except Exception as e2:
                                print("[main] åˆ‡æ¢åˆ°å¤‡ç”¨æ¨¡å‹åé‡è¯•ä»å¤±è´¥:", e2)
                                traceback.print_exc()
                                switched = False

                        if not switched:
                            # å°†å‰3æ¡æ£€ç´¢åˆ°çš„æ–‡æ¡£æ‹¼æ¥ä¸ºä¸´æ—¶å†…å®¹
                            concatenated = "\n\n".join([n.node.text for n in filtered_nodes[:3]])

                            # å°è¯•ä½¿ç”¨æœ¬åœ°å°æ¨¡å‹åšå¿«é€Ÿæ‘˜è¦ï¼ˆå¦‚æœé…ç½®å¹¶å­˜åœ¨æœ¬åœ°æ¨¡å‹ï¼‰
                            summary_text = None
                            try:
                                local_cfg = LLM_CONFIGS.get("local")
                                if local_cfg:
                                    local_model_path = local_cfg.get("model")
                                    if local_model_path and Path(local_model_path).exists():
                                        try:
                                            hf_llm = HuggingFaceLLM(model_name=str(local_model_path), temperature=0.2, max_length=256)
                                            # ä½¿ç”¨hf_llmè¿›è¡Œå¿«é€Ÿæ‘˜è¦
                                            summary_text = hf_llm.predict(f"è¯·ç®€è¦æ€»ç»“ä»¥ä¸‹æ³•å¾‹æ¡æ–‡è¦ç‚¹ï¼š\n\n{concatenated}\n\næ€»ç»“ï¼š")
                                        except Exception as e_local:
                                            print(f"[fallback] æœ¬åœ°æ¨¡å‹æ‘˜è¦å¤±è´¥: {e_local}")
                            except Exception as e_cfg:
                                print(f"[fallback] æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e_cfg}")

                            if summary_text:
                                cleaned_response = summary_text
                                response_text = f"âš ï¸ åç«¯æœåŠ¡å¼‚å¸¸ï¼Œä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆçš„ä¸´æ—¶æ‘˜è¦ï¼š\n\n{summary_text}"
                            else:
                                # å›é€€åˆ°æ‹¼æ¥çš„åŸæ–‡
                                cleaned_response = concatenated
                                response_text = f"âš ï¸ åç«¯æ¨¡å‹æœåŠ¡å¼‚å¸¸ï¼š{e}\n\nç›¸å…³æ¡æ–‡ï¼ˆä¸´æ—¶ç»“æœï¼‰ï¼š\n{concatenated}"
            
            # æ˜¾ç¤ºå›ç­”
            with st.chat_message("assistant"):
                # æå–æ€ç»´é“¾å†…å®¹å¹¶æ¸…ç†å“åº”æ–‡æœ¬
                think_contents = re.findall(r'<think>(.*?)</think>', response_text, re.DOTALL)
                cleaned_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()
                
                # æ˜¾ç¤ºæ¸…ç†åçš„å›ç­”
                st.markdown(cleaned_response)
                
                # å¦‚æœæœ‰æ€ç»´é“¾å†…å®¹åˆ™æ˜¾ç¤º
                if think_contents:
                    with st.expander("ğŸ“ æ¨¡å‹æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                        for content in think_contents:
                            st.markdown(f'<span style="color: #808080">{content.strip()}</span>', 
                                      unsafe_allow_html=True)
                
                # ä»…åœ¨æœ‰å‚è€ƒä¾æ®æ—¶æ˜¾ç¤ºï¼ˆæ³•å¾‹ç›¸å…³é—®é¢˜æ‰æœ‰å‚è€ƒä¾æ®ï¼‰
                if filtered_nodes:
                    show_reference_details(filtered_nodes[:3])

            # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²ï¼ˆéœ€è¦å­˜å‚¨åŸå§‹å“åº”ï¼‰
            st.session_state.messages.append({
                "role": "assistant",
                "content": response_text,  # ä¿ç•™åŸå§‹å“åº”
                "cleaned": cleaned_response,  # å­˜å‚¨æ¸…ç†åçš„æ–‡æœ¬
                "think": think_contents,  # å­˜å‚¨æ€ç»´é“¾å†…å®¹
                "reference_nodes": filtered_nodes[:3]  # å­˜å‚¨å‚è€ƒèŠ‚ç‚¹
            })

if __name__ == "__main__":
    main()